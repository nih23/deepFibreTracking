{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os, sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from collections import deque \n",
    "\n",
    "from dfibert.tracker.nn.rl import Agent, Action_Scheduler, DQN\n",
    "import dfibert.envs.RLtractEnvironment as RLTe\n",
    "from dfibert.envs._state import TractographyState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Lunar Lander to check functionality of agent\n",
    "#env = gym.make('LunarLander-v2')\n",
    "#n_actions= env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3000000\n",
    "replay_memory_size = 300000\n",
    "agent_history_length = 1\n",
    "evaluate_every = 10000\n",
    "eval_runs = 20\n",
    "network_update_every = 1500\n",
    "start_learning = 2000\n",
    "eps_annealing_steps = 150000\n",
    "\n",
    "max_episode_length = 250\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#batch_size = 64\n",
    "#learning_rate = 0.0000000625\n",
    "batch_size = 512\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed streamlines (data/HCP307200_DTI_smallSet.vtk) for ID 100307\n"
     ]
    }
   ],
   "source": [
    "env = RLTe.RLtractEnvironment(stepWidth=0.81, device = 'cpu')\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_transition():\n",
    "    state = env.reset()[:2]\n",
    "    #transition = deque(maxlen=12)\n",
    "    #while len(transition) < 12:\n",
    "        #for i in range(len(state.getCoordinate())):\n",
    "        #    transition.append(state.getCoordinate()[i].item())\n",
    "    transition = deque(maxlen=8)\n",
    "    while len(transition) < 8:\n",
    "        for i in range(len(state)):\n",
    "            transition.append(state[i])\n",
    "    return transition\n",
    "\n",
    "def add_to_transition(state, transition):\n",
    "    #for i in range(len(state.getCoordinate())):\n",
    "    #        transition.append(state.getCoordinate()[i].item())\n",
    "    for i in range(len(state)):\n",
    "            transition.append(state[i])                   \n",
    "    return transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([46.1923, 58.1386, 18.6595])\n",
      "[46.192337 58.13857  18.659458  0.      ]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state.getCoordinate())\n",
    "current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "qry_pt = state.getCoordinate().view(-1,3)\n",
    "distance = torch.sum((env.referenceStreamline_ijk[current_index] - qry_pt)**2)\n",
    "state = np.array([*state.getCoordinate(), distance])\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([73.6513442993164, 107.88105773925781, 93.29415130615234, 73.6513442993164, 107.88105773925781, 93.29415130615234, 73.6513442993164, 107.88105773925781, 93.29415130615234, 73.6513442993164, 107.88105773925781, 93.29415130615234], maxlen=12)\n",
      "[ 73.6513443  107.88105774  93.29415131  73.6513443  107.88105774\n",
      "  93.29415131  73.6513443  107.88105774  93.29415131  74.58926433\n",
      " 108.1431821   93.52130066]\n"
     ]
    }
   ],
   "source": [
    "transition = init_transition()\n",
    "print(transition)\n",
    "next_state, _, _ = env.step(42)\n",
    "next_transition = add_to_transition(next_state, transition)\n",
    "print(np.array(next_transition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "tensor([[ 61.8814,  53.5273,  65.1122,  59.5295,  63.2473,  63.3582,  49.6618,\n",
      "          57.5226,  56.9831,  61.0424,  49.2360,  45.0796,  60.3465,  55.7477,\n",
      "          60.6391,  54.9909,  55.8694,  60.2220,  35.0820,  47.0737,  53.0218,\n",
      "          56.9755,  60.0529,  32.3047,  20.4098,  58.4533,  59.7584,  63.0841,\n",
      "          54.4836,  43.6310,  57.3373,  22.6192,  32.9992,  61.3936,  32.4899,\n",
      "          60.6012,   8.0376,  39.1546,  58.7364,  28.7151,  25.3395,  59.5876,\n",
      "          13.1139,  54.3735,  25.8479,  10.7677,  54.2027,  11.2558,  38.6437,\n",
      "          60.1825,  22.2333,  24.1235,   8.7726,  27.5614,   2.9120,  12.3545,\n",
      "          26.7937,  17.6043,  34.9672, -12.4343,  23.0219,  27.1385, -15.7887,\n",
      "          61.7124,  42.1585,  20.3453,  24.3167,  -4.8190,  43.8532,  45.2633,\n",
      "          13.3825,  64.5126,   0.7540,  17.9066,  45.4248,  -8.3953,  60.9557,\n",
      "          51.1121,  22.9783,  49.9606,   8.7290,   3.0979,  55.1753,  -3.8223,\n",
      "          64.8611,  28.4904,  43.0782,  48.9762,  12.0101,  67.0276,  12.0111,\n",
      "          23.9232,  59.5696,  21.2108,  39.0073,  46.0813,  14.7475,  61.6706,\n",
      "          39.2160,  29.3636,  35.8707]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[ 63.6283,  55.3608,  66.8185,  61.1888,  66.2913,  66.0564,  51.9236,\n",
      "          59.3875,  59.3944,  63.5277,  53.6934,  48.2030,  63.8540,  58.4446,\n",
      "          63.3026,  58.4384,  57.4725,  62.1727,  37.2078,  49.2459,  56.9622,\n",
      "          60.5168,  62.6494,  35.4438,  22.5426,  62.1371,  62.0415,  65.8335,\n",
      "          57.5910,  46.8625,  60.4251,  26.2690,  36.7441,  63.1429,  35.5379,\n",
      "          64.3739,  12.9968,  41.9775,  61.7111,  33.1583,  29.6506,  61.3467,\n",
      "          15.9077,  56.4057,  30.3011,  13.3768,  56.7660,  14.4481,  40.0194,\n",
      "          63.0712,  27.3223,  26.0509,  11.5913,  33.8582,  10.7094,  15.7474,\n",
      "          31.1777,  20.3366,  38.5003,  -7.2658,  26.0212,  29.5287, -11.0445,\n",
      "          66.1286,  46.5212,  23.9947,  28.5018,   0.2056,  49.2398,  46.6589,\n",
      "          17.8200,  68.1692,   5.3408,  20.7294,  46.7196,  -2.4405,  65.7757,\n",
      "          53.7897,  25.3095,  52.4488,  13.9321,   5.8551,  57.1451,   0.3461,\n",
      "          71.6645,  32.7234,  46.6471,  51.2306,  16.3589,  71.9192,  16.1005,\n",
      "          27.7186,  63.3874,  24.3338,  41.3170,  49.6902,  19.2401,  71.4144,\n",
      "          42.2975,  31.2736,  38.9589]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "<dfibert.envs._state.TractographyState object at 0x2aaab54c11c0> 0.9 False\n",
      "tensor([[ 61.8814,  53.5273,  65.1122,  59.5295,  63.2473,  63.3582,  49.6618,\n",
      "          57.5226,  56.9831,  61.0424,  49.2360,  45.0796,  60.3465,  55.7477,\n",
      "          60.6391,  54.9909,  55.8694,  60.2220,  35.0820,  47.0737,  53.0218,\n",
      "          56.9755,  60.0529,  32.3047,  20.4098,  58.4533,  59.7584,  63.0841,\n",
      "          54.4836,  43.6310,  57.3373,  22.6192,  32.9992,  61.3936,  32.4899,\n",
      "          60.6012,   8.0376,  39.1546,  58.7364,  28.7151,  25.3395,  59.5876,\n",
      "          13.1139,  54.3735,  25.8479,  10.7677,  54.2027,  11.2558,  38.6437,\n",
      "          60.1825,  22.2333,  24.1235,   8.7726,  27.5614,   2.9120,  12.3545,\n",
      "          26.7937,  17.6043,  34.9672, -12.4343,  23.0219,  27.1385, -15.7887,\n",
      "          61.7124,  42.1585,  20.3453,  24.3167,  -4.8190,  43.8532,  45.2633,\n",
      "          13.3825,  64.5126,   0.7540,  17.9066,  45.4248,  -8.3953,  60.9557,\n",
      "          51.1121,  22.9783,  49.9606,   8.7290,   3.0979,  55.1753,  -3.8223,\n",
      "          64.8611,  28.4904,  43.0782,  48.9762,  12.0101,  67.0276,  12.0111,\n",
      "          23.9232,  59.5696,  21.2108,  39.0073,  46.0813,  14.7475,  61.6706,\n",
      "          39.2160,  29.3636,  35.8707]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([53.5273], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "Argmax actions:  tensor([2], device='cuda:0')\n",
      "tensor([54.0936], device='cuda:0', grad_fn=<IndexBackward>)\n",
      "tensor([54.0936], device='cuda:0', grad_fn=<IndexPutBackward>)\n",
      "tensor([54.4527], device='cuda:0')\n",
      "tensor(0.4282, device='cuda:0', grad_fn=<SmoothL1LossBackward>)\n",
      "tensor([[69.8217, 62.7116, 72.9487, 67.3711, 70.8939, 71.1167, 57.6730, 65.5343,\n",
      "         64.8148, 68.7852, 56.6590, 52.5855, 67.8952, 63.3010, 68.3314, 62.3348,\n",
      "         63.6183, 67.6165, 42.4274, 54.9824, 60.3265, 64.3770, 67.3753, 39.9209,\n",
      "         28.0862, 65.7306, 67.4345, 70.9891, 61.7090, 51.1367, 64.6862, 29.8620,\n",
      "         40.6824, 68.7255, 39.7542, 67.9210, 15.2490, 46.6965, 66.0314, 35.6698,\n",
      "         32.9140, 66.8376, 20.4195, 61.6589, 32.8767, 18.2042, 61.3629, 18.4511,\n",
      "         46.3521, 67.3684, 28.9866, 31.0500, 15.8538, 34.3417,  9.6600, 19.4795,\n",
      "         33.3326, 25.0639, 42.1582, -5.5555, 30.0898, 34.0673, -8.8221, 68.8530,\n",
      "         49.1213, 27.6719, 31.1747,  2.1985, 50.7622, 52.4861, 20.5302, 71.4381,\n",
      "          7.7528, 25.3170, 52.6153, -1.3586, 68.0895, 58.1571, 30.3286, 57.2420,\n",
      "         15.6469, 10.5821, 62.2527,  3.3277, 71.8068, 35.5090, 50.3183, 56.1360,\n",
      "         19.3133, 73.9992, 18.8239, 31.1360, 66.8728, 28.4109, 46.6219, 53.2537,\n",
      "         22.0076, 67.9159, 46.1737, 36.9736, 43.1922]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[ 63.6283,  55.3608,  66.8185,  61.1888,  66.2913,  66.0564,  51.9236,\n",
      "          59.3875,  59.3944,  63.5277,  53.6934,  48.2030,  63.8540,  58.4446,\n",
      "          63.3026,  58.4384,  57.4725,  62.1727,  37.2078,  49.2459,  56.9622,\n",
      "          60.5168,  62.6494,  35.4438,  22.5426,  62.1371,  62.0415,  65.8335,\n",
      "          57.5910,  46.8625,  60.4251,  26.2690,  36.7441,  63.1429,  35.5379,\n",
      "          64.3739,  12.9968,  41.9775,  61.7111,  33.1583,  29.6506,  61.3467,\n",
      "          15.9077,  56.4057,  30.3011,  13.3768,  56.7660,  14.4481,  40.0194,\n",
      "          63.0712,  27.3223,  26.0509,  11.5913,  33.8582,  10.7094,  15.7474,\n",
      "          31.1777,  20.3366,  38.5003,  -7.2658,  26.0212,  29.5287, -11.0445,\n",
      "          66.1286,  46.5212,  23.9947,  28.5018,   0.2056,  49.2398,  46.6589,\n",
      "          17.8200,  68.1692,   5.3408,  20.7294,  46.7196,  -2.4405,  65.7757,\n",
      "          53.7897,  25.3095,  52.4488,  13.9321,   5.8551,  57.1451,   0.3461,\n",
      "          71.6645,  32.7234,  46.6471,  51.2306,  16.3589,  71.9192,  16.1005,\n",
      "          27.7186,  63.3874,  24.3338,  41.3170,  49.6902,  19.2401,  71.4144,\n",
      "          42.2975,  31.2736,  38.9589]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Debugging the optimization of the agent\n",
    "\n",
    "#state = env.reset()\n",
    "#state = torch.tensor([state]).to(device).float()\n",
    "transition = init_transition()\n",
    "state = torch.FloatTensor([np.array(transition)]).to(device)\n",
    "print(state.shape)\n",
    "print(agent.main_dqn(state))\n",
    "print(agent.target_dqn(state))\n",
    "\n",
    "action = 1\n",
    "\n",
    "next_state, reward, done = env.step(action)\n",
    "print(next_state, reward, done)\n",
    "\n",
    "action = torch.tensor([action]).to(device)\n",
    "#next_state = torch.tensor([next_state]).float().to(device)\n",
    "next_state = add_to_transition(next_state, transition)\n",
    "next_state = torch.FloatTensor([np.array(next_state)]).to(device)\n",
    "reward = torch.tensor([reward]).float().to(device)\n",
    "done = torch.BoolTensor([done]).to(device)\n",
    "state_action_values = agent.main_dqn(state)\n",
    "print(state_action_values)\n",
    "state_action_values = state_action_values[0][action]\n",
    "print(state_action_values)\n",
    "\n",
    "next_state_actions = agent.main_dqn(next_state).max(1)[1]\n",
    "print(\"Argmax actions: \", next_state_actions)\n",
    "next_state_values = agent.target_dqn(next_state)[0][next_state_actions]\n",
    "print(next_state_values)\n",
    "next_state_values[done] = 0.0\n",
    "print(next_state_values)\n",
    "expected_state_action_values = next_state_values.detach() * 0.99 + reward\n",
    "print(expected_state_action_values)\n",
    "\n",
    "agent.optimizer.zero_grad()\n",
    "loss = torch.nn.SmoothL1Loss()(state_action_values, expected_state_action_values)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "agent.optimizer.step()\n",
    "\n",
    "print(agent.main_dqn(state))\n",
    "print(agent.target_dqn(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "def get_random_good_action(state, env):\n",
    "    best_actions = []\n",
    "    #path_vectors = []\n",
    "    #reference_vectors = []\n",
    "    #cosine_sims = []\n",
    "    #distances = []\n",
    "    rewards = []\n",
    "    for i in range(n_actions):\n",
    "        #print(state.getCoordinate(), env.state.getCoordinate())\n",
    "        #print(env.stepCounter)\n",
    "        next_state, _,_ = env.step(i)\n",
    "        #print(env.stepCounter)\n",
    "        current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "        #print(current_index)\n",
    "        path_vector = next_state.getCoordinate() - state.getCoordinate().squeeze(0)\n",
    "        #print(path_vector)\n",
    "        #path_vectors.append(path_vector.numpy())\n",
    "        reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "        #print(reference_vector)\n",
    "        #reference_vectors.append(reference_vector.numpy())\n",
    "        cosine_sim = cos(path_vector, reference_vector)\n",
    "        #cosine_sims.append(cosine_sim.item())\n",
    "        #print(cosine_sim)\n",
    "        dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2)\n",
    "        if dist < 0.1:\n",
    "            dist = 0\n",
    "        else:\n",
    "            dist = dist - 0.1\n",
    "        #print(dist)\n",
    "        #distances.append(dist.item())\n",
    "\n",
    "        reward = cosine_sim - dist\n",
    "        rewards.append(reward.item())\n",
    "        #print(reward)\n",
    "        best_actions.append(reward)\n",
    "        env.state = state\n",
    "        env.stepCounter -= 1\n",
    "\n",
    "    #best_actions = torch.topk(torch.tensor(best_actions), k=0)[1].numpy()\n",
    "    #random_action = np.random.choice(best_actions, size=1)\n",
    "    best_action= torch.argmax(torch.tensor(best_actions))\n",
    "    return best_action, rewards[best_action]#random_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: all CUDA-capable devices are busy or unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-03b43ac7a5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_history_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent_history_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplay_memory_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moverall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moverall_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/tracker/nn/rl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_actions, device, inp_size, hidden, learning_rate, gamma, batch_size, agent_history_length, memory_size)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Create 2 models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minp_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# and send them to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: all CUDA-capable devices are busy or unavailable"
     ]
    }
   ],
   "source": [
    "state = env.reset().getValue()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.9, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=32, learning_rate=learning_rate)\n",
    "\n",
    "overall_runs = 0\n",
    "overall_reward = []\n",
    "while agent.replay_memory.current < 2000:\n",
    "    state = env.reset()\n",
    "    #episode_step_counter = 0\n",
    "    episode_reward = 0\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        #print(env.stepCounter)\n",
    "        action, optimal_reward = get_random_good_action(state)\n",
    "        if np.random.rand(1) < 0.1: \n",
    "            action = np.random.randint(0, n_actions)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        terminal = False\n",
    "\n",
    "        current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "        path_vector = (next_state.getCoordinate() - state.getCoordinate()).squeeze(0)\n",
    "        reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "        #    #print(path_vector, reference_vector)\n",
    "        cosine_sim = cos(path_vector, reference_vector)\n",
    "        dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2).clamp_(0,-2)\n",
    "        if dist < 0.1:\n",
    "            dist = 0\n",
    "        else:\n",
    "            dist = dist - 0.1\n",
    "        reward = cosine_sim - dist\n",
    "        reward = 1 - (optimal_reward - reward)\n",
    "        if action == 100 and reward == 1:\n",
    "            terminal = True\n",
    "\n",
    "        if env.stepCounter == 200:\n",
    "            terminal = True\n",
    "            \n",
    "        agent.replay_memory.add_experience(action=action,\n",
    "                                state = state.getValue(),\n",
    "                                reward=reward,\n",
    "                                new_state = next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if terminal == True:\n",
    "            overall_runs += 1\n",
    "            overall_reward.append(episode_reward)\n",
    "            print(overall_runs, np.mean(overall_reward[-100:]))\n",
    "print(\"Replay memory ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, next_states, terminal_flags = agent.replay_memory.get_minibatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 -33.590794\n",
      "47 1.0\n",
      "66 1.0\n",
      "35 1.0\n",
      "21 1.0\n",
      "58 1.0\n",
      "48 -98.96326\n",
      "97 1.0\n",
      "39 1.0\n",
      "86 -114.20049\n",
      "15 1.0\n",
      "23 1.0\n",
      "89 1.0\n",
      "13 1.0\n",
      "74 1.0\n",
      "23 1.0\n",
      "34 1.0\n",
      "100 1.0\n",
      "12 1.0\n",
      "16 1.0\n",
      "67 -90.898346\n",
      "66 1.0\n",
      "77 1.0\n",
      "20 1.0\n",
      "84 1.0\n",
      "65 1.0\n",
      "75 1.0\n",
      "81 -78.17227\n",
      "100 1.0\n",
      "47 1.0\n",
      "38 1.0\n",
      "8 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(actions)):\n",
    "    \n",
    "    print(actions[i], rewards[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26) 0.9905484519844491 0.012349212181913333 tensor(1., dtype=torch.float64)\n",
      "tensor(21) 0.9950890785943444 0.01679870512867484 tensor(1., dtype=torch.float64)\n",
      "tensor(21) 0.992682121479147 0.047323106923422745 tensor(1., dtype=torch.float64)\n",
      "tensor(21) 0.9851133177525272 0.00861279377772418 tensor(1., dtype=torch.float64)\n",
      "tensor(47) 0.9979308665390745 0.01409045354127273 tensor(1., dtype=torch.float64)\n",
      "tensor(93) 0.9935704882272718 0.03343560736630453 tensor(1., dtype=torch.float64)\n",
      "tensor(93) 0.99009462013209 0.049526562098364015 tensor(1., dtype=torch.float64)\n",
      "tensor(39) 0.9832455250018713 0.010603841726498708 tensor(1., dtype=torch.float64)\n",
      "tensor(39) 0.9832469051382827 0.015307816801382905 tensor(1., dtype=torch.float64)\n",
      "tensor(93) 0.9773084764540848 0.012741049264985184 tensor(1., dtype=torch.float64)\n",
      "tensor(39) 0.9870199722179482 0.00883634737326538 tensor(1., dtype=torch.float64)\n",
      "tensor(47) 0.9724302288777857 0.020489734216548695 tensor(1., dtype=torch.float64)\n",
      "tensor(39) 0.9715788184157799 0.022957109141215945 tensor(1., dtype=torch.float64)\n",
      "tensor(26) 0.9987427858422983 0.01670007901487542 tensor(1., dtype=torch.float64)\n",
      "tensor(13) 0.9689588166805283 0.006675853429276799 tensor(1., dtype=torch.float64)\n",
      "tensor(26) 0.9905491327702011 0.0018542543465868357 tensor(1., dtype=torch.float64)\n",
      "tensor(13) 0.9935575661897293 0.008063889400441022 tensor(1., dtype=torch.float64)\n",
      "tensor(26) 0.9781637939053769 0.00934661801865385 tensor(1., dtype=torch.float64)\n",
      "tensor(13) 0.9819791156060025 0.005152187815845708 tensor(1., dtype=torch.float64)\n",
      "tensor(26) 0.9781635014381644 0.02520498996268137 tensor(1., dtype=torch.float64)\n",
      "tensor(13) 0.981978134295604 0.02988953189363085 tensor(1., dtype=torch.float64)\n",
      "tensor(18) 0.9650815981158287 0.0029102219285672168 tensor(1., dtype=torch.float64)\n",
      "tensor(26) 0.9743095472962682 0.018294755261070633 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9822298610865827 0.008992112325095808 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9958337781844809 0.011954325662145684 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9899274567397747 0.011108382153627005 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9899290997167907 0.03656649780648026 tensor(1., dtype=torch.float64)\n",
      "tensor(36) 0.9782931763485482 0.022380203966198872 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9807951214231843 0.015291410061735625 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.979215435797092 0.03304997813266605 tensor(1., dtype=torch.float64)\n",
      "tensor(15) 0.9859963882590943 0.017034464650167752 tensor(1., dtype=torch.float64)\n",
      "tensor(15) 0.9859963882590943 0.03751622359684466 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9792146983949596 0.020853578613655582 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9979780983277579 0.030543201393183752 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9954062739034413 0.01059341620882976 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9822298610865827 0.00730790594092138 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9954062739034413 0.014834170474659156 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9954066921418483 0.03446734824411912 tensor(1., dtype=torch.float64)\n",
      "tensor(18) 0.9620275710548547 0.0037674763828573635 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9954066921418483 0.0023188676670822935 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9867304388372005 0.020909883688113837 tensor(1., dtype=torch.float64)\n",
      "tensor(10) 0.9667780246515739 0.00585178087614638 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9867304388372005 0.008019893481991534 tensor(1., dtype=torch.float64)\n",
      "tensor(23) 0.9867295434983365 0.04478446152149267 tensor(1., dtype=torch.float64)\n",
      "tensor(10) 0.966776260864322 0.014000612218098127 tensor(1., dtype=torch.float64)\n",
      "tensor(7) 0.98989635344426 0.00026449282221202634 tensor(1., dtype=torch.float64)\n",
      "tensor(4) 0.9908904268547565 0.01108276406430656 tensor(1., dtype=torch.float64)\n",
      "tensor(17) 0.9877991434569441 0.037171035686342974 tensor(1., dtype=torch.float64)\n",
      "tensor(35) 0.9788731731621277 0.04087709637317305 tensor(1., dtype=torch.float64)\n",
      "tensor(43) 0.9900946201320904 0.03189983958849383 tensor(1., dtype=torch.float64)\n",
      "tensor(43) 0.9900946201320904 0.04879732384958244 tensor(1., dtype=torch.float64)\n",
      "tensor(89) 0.9535263515280072 0.040688621940667816 tensor(1., dtype=torch.float64)\n",
      "tensor(43) 0.9900934538224782 0.05633317651074972 tensor(1., dtype=torch.float64)\n",
      "tensor(22) 0.9769329331636237 0.052200114258105425 tensor(1., dtype=torch.float64)\n",
      "tensor(22) 0.9743220410041671 0.017908348216813988 tensor(1., dtype=torch.float64)\n",
      "tensor(17) 0.9877991434569441 0.03615960431266288 tensor(1., dtype=torch.float64)\n",
      "tensor(4) 0.9886056953962735 0.047688209618090255 tensor(1., dtype=torch.float64)\n",
      "tensor(12) 0.9630172011497639 0.018379298127968728 tensor(1., dtype=torch.float64)\n",
      "tensor(12) 0.9946887145656262 0.03994299066447957 tensor(1., dtype=torch.float64)\n",
      "tensor(4) 0.9445606297816944 0.0071251374870612795 tensor(1., dtype=torch.float64)\n",
      "tensor(12) 0.9988977482470188 0.012038026401231042 tensor(1., dtype=torch.float64)\n",
      "tensor(12) 0.9988982530020647 0.020005299435204422 tensor(1., dtype=torch.float64)\n",
      "tensor(12) 0.9988982530020647 0.031028351334061666 tensor(1., dtype=torch.float64)\n",
      "tensor(12) 0.9942110228060098 0.015609584871018217 tensor(1., dtype=torch.float64)\n",
      "tensor(12) 0.9791076622944607 0.006760345245118361 tensor(1., dtype=torch.float64)\n",
      "tensor(4) 0.9946086517139643 0.02602214542604791 tensor(1., dtype=torch.float64)\n",
      "tensor(0) 0.9800823453542437 0.022013549620405436 tensor(1., dtype=torch.float64)\n",
      "tensor(1) 0.9648375035657275 0.012270519107677517 tensor(1., dtype=torch.float64)\n",
      "tensor(0) 0.9824758786724938 0.007579914715997716 tensor(1., dtype=torch.float64)\n",
      "tensor(1) 0.9786475341707532 0.009389215402735294 tensor(1., dtype=torch.float64)\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "tensor(0) 0.9824752060068921 0.70037538453382 tensor(1., dtype=torch.float64)\n",
      "tensor(100) 0.0 0.70037538453382 tensor(1., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "terminal = False\n",
    "all_states = []\n",
    "all_states.append(state.getCoordinate())\n",
    "\n",
    "while not terminal:\n",
    "    action, optimal_reward  = get_random_good_action(state, env)\n",
    "    #if np.random.rand(1) < 0.1: \n",
    "    #    action = np.random.randint(0, n_actions)\n",
    "    #print(\"Action:\", action)\n",
    "    next_state, _,_ = env.step(action)\n",
    "    #print(\"Step counter: \", env.stepCounter)\n",
    "    current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "    #print(\"Current index: \", current_index)\n",
    "    #print(\"state.getCoordinate(): \", state.getCoordinate().numpy())\n",
    "    #print(\"env.state.getCoordinate(): \", env.state.getCoordinate().numpy())\n",
    "    #print(\"next_state.getCoordinate(): \", next_state.getCoordinate().numpy())\n",
    "    path_vector = next_state.getCoordinate() - state.getCoordinate().squeeze(0)\n",
    "    #print(\"path vector: \", path_vector)\n",
    "    reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "    #print(\"reference_vector: \", reference_vector)\n",
    "    cosine_sim = cos(path_vector, reference_vector)\n",
    "    #print(\"cosine_sim: \", cosine_sim)\n",
    "    dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2)\n",
    "    if dist > 3*0.81:\n",
    "        env.stepCounter -= 1\n",
    "    #print(\"distance: \", dist)\n",
    "    all_states.append(next_state.getCoordinate())\n",
    "    state = next_state\n",
    "    print(action, cosine_sim.item(), dist.item(), 1-(optimal_reward-(cosine_sim-dist)))\n",
    "    if action == 100 and 1-(optimal_reward-(cosine_sim-dist)) == 1:\n",
    "        terminal = True\n",
    "    else:\n",
    "        terminal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "200, done 1 episodes, -22296.07382175492, current eps 1 200.0\n",
      "400, done 2 episodes, -12849.793506910173, current eps 1 200.0\n",
      "600, done 3 episodes, -11116.709426083376, current eps 1 200.0\n",
      "800, done 4 episodes, -11707.445859817346, current eps 1 200.0\n",
      "1000, done 5 episodes, -10979.221595952506, current eps 1 200.0\n",
      "1200, done 6 episodes, -10037.890999587267, current eps 1 200.0\n",
      "1400, done 7 episodes, -9919.901146263192, current eps 1 200.0\n",
      "1600, done 8 episodes, -9375.372679725791, current eps 1 200.0\n",
      "1800, done 9 episodes, -9448.01685163361, current eps 1 200.0\n",
      "2000, done 10 episodes, -9736.256351331944, current eps 1 200.0\n",
      "2200, done 11 episodes, -9517.97880361734, current eps 0.9989386666666666 200.0\n",
      "2400, done 12 episodes, -9223.795990676945, current eps 0.997872 200.0\n",
      "2600, done 13 episodes, -9671.24654551019, current eps 0.9968053333333333 200.0\n",
      "2800, done 14 episodes, -9263.060011922971, current eps 0.9957386666666665 200.0\n",
      "3000, done 15 episodes, -9029.091847713686, current eps 0.9946719999999999 200.0\n",
      "3200, done 16 episodes, -9141.68305898002, current eps 0.9936053333333332 200.0\n",
      "3400, done 17 episodes, -9048.960342088007, current eps 0.9925386666666666 200.0\n",
      "3600, done 18 episodes, -8871.057332807757, current eps 0.9914719999999999 200.0\n",
      "3800, done 19 episodes, -8652.642091134234, current eps 0.9904053333333332 200.0\n",
      "4000, done 20 episodes, -8300.334737234367, current eps 0.9893386666666666 200.0\n",
      "4200, done 21 episodes, -8191.453775867126, current eps 0.9882719999999999 200.0\n",
      "4400, done 22 episodes, -8034.176004728762, current eps 0.9872053333333333 200.0\n",
      "4600, done 23 episodes, -7925.1091677844115, current eps 0.9861386666666666 200.0\n",
      "4800, done 24 episodes, -8473.1001783564, current eps 0.985072 200.0\n",
      "5000, done 25 episodes, -9248.603139203195, current eps 0.9840053333333333 200.0\n",
      "5200, done 26 episodes, -9058.74744524469, current eps 0.9829386666666666 200.0\n",
      "5400, done 27 episodes, -8891.304019284093, current eps 0.981872 200.0\n",
      "5600, done 28 episodes, -8700.57310846713, current eps 0.9808053333333333 200.0\n",
      "5800, done 29 episodes, -8693.46786581214, current eps 0.9797386666666666 200.0\n",
      "6000, done 30 episodes, -9026.496785861482, current eps 0.978672 200.0\n",
      "6200, done 31 episodes, -9542.088673144663, current eps 0.9776053333333332 200.0\n",
      "6400, done 32 episodes, -10097.123541070563, current eps 0.9765386666666666 200.0\n",
      "6600, done 33 episodes, -9867.78005722657, current eps 0.9754719999999999 200.0\n",
      "6800, done 34 episodes, -9704.158762271973, current eps 0.9744053333333332 200.0\n",
      "7000, done 35 episodes, -9637.539621831727, current eps 0.9733386666666666 200.0\n",
      "7200, done 36 episodes, -9589.51370561506, current eps 0.9722719999999999 200.0\n",
      "7400, done 37 episodes, -9369.336829912365, current eps 0.9712053333333333 200.0\n",
      "7600, done 38 episodes, -9180.174642229129, current eps 0.9701386666666666 200.0\n",
      "7800, done 39 episodes, -9190.327846789605, current eps 0.9690719999999999 200.0\n",
      "8000, done 40 episodes, -9061.062328246666, current eps 0.9680053333333333 200.0\n",
      "8200, done 41 episodes, -9464.59245292344, current eps 0.9669386666666666 200.0\n",
      "8400, done 42 episodes, -9312.957211948633, current eps 0.965872 200.0\n",
      "8600, done 43 episodes, -9134.887342267455, current eps 0.9648053333333333 200.0\n",
      "8800, done 44 episodes, -9039.803145391264, current eps 0.9637386666666666 200.0\n",
      "9000, done 45 episodes, -9019.20193877639, current eps 0.962672 200.0\n",
      "9200, done 46 episodes, -9037.713992256717, current eps 0.9616053333333333 200.0\n",
      "9400, done 47 episodes, -9024.046730236285, current eps 0.9605386666666667 200.0\n",
      "9600, done 48 episodes, -9253.044281733595, current eps 0.9594719999999999 200.0\n",
      "9800, done 49 episodes, -9123.677787285495, current eps 0.9584053333333332 200.0\n",
      "10000, done 50 episodes, -9088.177874045317, current eps 0.9573386666666666 200.0\n",
      "Evaluation score: -0.32957014408246044\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "10200, done 51 episodes, -9145.258533138023, current eps 0.9562719999999999 200.0\n",
      "10400, done 52 episodes, -9126.785012783466, current eps 0.9552053333333332 200.0\n",
      "10600, done 53 episodes, -8996.35844007913, current eps 0.9541386666666666 200.0\n",
      "10800, done 54 episodes, -8919.888650118555, current eps 0.9530719999999999 200.0\n",
      "11000, done 55 episodes, -9221.335651689737, current eps 0.9520053333333333 200.0\n",
      "11200, done 56 episodes, -9135.932096479544, current eps 0.9509386666666666 200.0\n",
      "11400, done 57 episodes, -9174.049686985212, current eps 0.9498719999999999 200.0\n",
      "11600, done 58 episodes, -9059.329809330007, current eps 0.9488053333333333 200.0\n",
      "11800, done 59 episodes, -8952.219140370386, current eps 0.9477386666666666 200.0\n",
      "12000, done 60 episodes, -9017.249882341488, current eps 0.946672 200.0\n",
      "12200, done 61 episodes, -8971.450712463113, current eps 0.9456053333333333 200.0\n",
      "12400, done 62 episodes, -8879.0328764603, current eps 0.9445386666666666 200.0\n",
      "12600, done 63 episodes, -8764.900020792677, current eps 0.943472 200.0\n",
      "12800, done 64 episodes, -8671.878003659973, current eps 0.9424053333333333 200.0\n",
      "13000, done 65 episodes, -8664.903887414606, current eps 0.9413386666666665 200.0\n",
      "13200, done 66 episodes, -8558.640330257504, current eps 0.9402719999999999 200.0\n",
      "13400, done 67 episodes, -8479.279983551607, current eps 0.9392053333333332 200.0\n",
      "13600, done 68 episodes, -8428.902417157466, current eps 0.9381386666666666 200.0\n",
      "13800, done 69 episodes, -8427.401394445826, current eps 0.9370719999999999 200.0\n",
      "14000, done 70 episodes, -8397.854158747252, current eps 0.9360053333333332 200.0\n",
      "14200, done 71 episodes, -8405.724982421969, current eps 0.9349386666666666 200.0\n",
      "14400, done 72 episodes, -8461.596836027788, current eps 0.9338719999999999 200.0\n",
      "14600, done 73 episodes, -8445.921729019861, current eps 0.9328053333333333 200.0\n",
      "14800, done 74 episodes, -8376.55414559934, current eps 0.9317386666666666 200.0\n",
      "15000, done 75 episodes, -8317.569805680441, current eps 0.9306719999999999 200.0\n",
      "15200, done 76 episodes, -8313.012618166773, current eps 0.9296053333333333 200.0\n",
      "15400, done 77 episodes, -8260.722133314162, current eps 0.9285386666666666 200.0\n",
      "15600, done 78 episodes, -8428.512119689873, current eps 0.927472 200.0\n",
      "15800, done 79 episodes, -8349.319629156444, current eps 0.9264053333333333 200.0\n",
      "16000, done 80 episodes, -8381.18946515871, current eps 0.9253386666666666 200.0\n",
      "16200, done 81 episodes, -8361.400400839413, current eps 0.924272 200.0\n",
      "16400, done 82 episodes, -8289.235888899379, current eps 0.9232053333333332 200.0\n",
      "16600, done 83 episodes, -8220.610308692327, current eps 0.9221386666666666 200.0\n",
      "16800, done 84 episodes, -8162.659844545517, current eps 0.9210719999999999 200.0\n",
      "17000, done 85 episodes, -8093.584378798918, current eps 0.9200053333333332 200.0\n",
      "17200, done 86 episodes, -8111.88046987617, current eps 0.9189386666666666 200.0\n",
      "17400, done 87 episodes, -8188.631308157523, current eps 0.9178719999999999 200.0\n",
      "17600, done 88 episodes, -8121.176402269495, current eps 0.9168053333333332 200.0\n",
      "17800, done 89 episodes, -8046.8102244206475, current eps 0.9157386666666666 200.0\n",
      "18000, done 90 episodes, -8188.843792982998, current eps 0.9146719999999999 200.0\n",
      "18200, done 91 episodes, -8145.242112547621, current eps 0.9136053333333333 200.0\n",
      "18400, done 92 episodes, -8284.314481063984, current eps 0.9125386666666666 200.0\n",
      "18600, done 93 episodes, -8216.039238809946, current eps 0.911472 200.0\n",
      "18800, done 94 episodes, -8176.7956939483765, current eps 0.9104053333333333 200.0\n",
      "19000, done 95 episodes, -8123.488572468198, current eps 0.9093386666666666 200.0\n",
      "19200, done 96 episodes, -8067.251329050644, current eps 0.908272 200.0\n",
      "19400, done 97 episodes, -8020.381449718696, current eps 0.9072053333333333 200.0\n",
      "19600, done 98 episodes, -7963.290167015352, current eps 0.9061386666666666 200.0\n",
      "19800, done 99 episodes, -7972.891884098732, current eps 0.9050719999999999 200.0\n",
      "20000, done 100 episodes, -8093.8901801568945, current eps 0.9040053333333332 200.0\n",
      "Evaluation score: -0.0916461608377159\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "20200, done 101 episodes, -7901.027843187895, current eps 0.9029386666666666 200.0\n",
      "20400, done 102 episodes, -7910.812925660055, current eps 0.9018719999999999 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20600, done 103 episodes, -7894.0402274466915, current eps 0.9008053333333332 200.0\n",
      "20800, done 104 episodes, -7858.7986000117935, current eps 0.8997386666666666 200.0\n",
      "21000, done 105 episodes, -7860.6886065599865, current eps 0.8986719999999999 200.0\n",
      "21200, done 106 episodes, -7840.411470994981, current eps 0.8976053333333333 200.0\n",
      "21400, done 107 episodes, -7818.356159012156, current eps 0.8965386666666666 200.0\n",
      "21600, done 108 episodes, -7811.771696806853, current eps 0.8954719999999999 200.0\n",
      "21800, done 109 episodes, -7853.560547712522, current eps 0.8944053333333333 200.0\n",
      "22000, done 110 episodes, -7768.024673341017, current eps 0.8933386666666666 200.0\n",
      "22200, done 111 episodes, -7971.740109605942, current eps 0.892272 200.0\n",
      "22400, done 112 episodes, -7925.560994330343, current eps 0.8912053333333333 200.0\n",
      "22600, done 113 episodes, -7886.752095024917, current eps 0.8901386666666666 200.0\n",
      "22800, done 114 episodes, -7947.567983189436, current eps 0.889072 200.0\n",
      "23000, done 115 episodes, -7944.858596913891, current eps 0.8880053333333333 200.0\n",
      "23200, done 116 episodes, -7869.5104671053305, current eps 0.8869386666666665 200.0\n",
      "23400, done 117 episodes, -7819.342094896403, current eps 0.8858719999999999 200.0\n",
      "23600, done 118 episodes, -7800.635502495785, current eps 0.8848053333333332 200.0\n",
      "23800, done 119 episodes, -7870.029221254743, current eps 0.8837386666666666 200.0\n",
      "24000, done 120 episodes, -8063.157555978424, current eps 0.8826719999999999 200.0\n",
      "24200, done 121 episodes, -8038.99344103975, current eps 0.8816053333333332 200.0\n",
      "24400, done 122 episodes, -8215.163099472391, current eps 0.8805386666666666 200.0\n",
      "24600, done 123 episodes, -8222.378462366596, current eps 0.8794719999999999 200.0\n",
      "24800, done 124 episodes, -8067.414910539468, current eps 0.8784053333333333 200.0\n",
      "25000, done 125 episodes, -7872.786230667681, current eps 0.8773386666666666 200.0\n",
      "25200, done 126 episodes, -7896.756809130171, current eps 0.8762719999999999 200.0\n",
      "25400, done 127 episodes, -7872.301711193655, current eps 0.8752053333333333 200.0\n",
      "25600, done 128 episodes, -7880.647415938882, current eps 0.8741386666666666 200.0\n",
      "25800, done 129 episodes, -7877.112491523727, current eps 0.873072 200.0\n",
      "26000, done 130 episodes, -7747.169700808145, current eps 0.8720053333333333 200.0\n",
      "26200, done 131 episodes, -7544.686429628033, current eps 0.8709386666666665 200.0\n",
      "26400, done 132 episodes, -7365.802865640278, current eps 0.869872 200.0\n",
      "26600, done 133 episodes, -7398.794233401544, current eps 0.8688053333333332 200.0\n",
      "26800, done 134 episodes, -7411.213775785458, current eps 0.8677386666666667 200.0\n",
      "27000, done 135 episodes, -7580.050558569333, current eps 0.8666719999999999 200.0\n",
      "27200, done 136 episodes, -7541.773323271857, current eps 0.8656053333333332 200.0\n",
      "27400, done 137 episodes, -7576.997844928542, current eps 0.8645386666666666 200.0\n",
      "27600, done 138 episodes, -7655.874875824154, current eps 0.8634719999999999 200.0\n",
      "27800, done 139 episodes, -7772.606911667518, current eps 0.8624053333333332 200.0\n",
      "28000, done 140 episodes, -7951.218418407839, current eps 0.8613386666666666 200.0\n",
      "28200, done 141 episodes, -7725.949615993708, current eps 0.8602719999999999 200.0\n",
      "28400, done 142 episodes, -7735.899688290321, current eps 0.8592053333333333 200.0\n",
      "28600, done 143 episodes, -7775.417251358125, current eps 0.8581386666666666 200.0\n",
      "28800, done 144 episodes, -7761.409403615457, current eps 0.857072 200.0\n",
      "29000, done 145 episodes, -7744.907689823599, current eps 0.8560053333333333 200.0\n",
      "29200, done 146 episodes, -7716.2047548819255, current eps 0.8549386666666666 200.0\n",
      "29400, done 147 episodes, -7838.409046406406, current eps 0.853872 200.0\n",
      "29600, done 148 episodes, -7668.318272033923, current eps 0.8528053333333332 200.0\n",
      "29800, done 149 episodes, -7698.661740217129, current eps 0.8517386666666666 200.0\n",
      "30000, done 150 episodes, -7668.619812031858, current eps 0.8506719999999999 200.0\n",
      "Evaluation score: 0.2704948870451581\n",
      "0 of 20 episodes ended close to / at the final state.\n",
      "30200, done 151 episodes, -7562.099339138394, current eps 0.8496053333333333 200.0\n",
      "30400, done 152 episodes, -7525.398493367113, current eps 0.8485386666666666 200.0\n",
      "30600, done 153 episodes, -7567.327459517693, current eps 0.8474719999999999 200.0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset().getValue()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.shape, device=device, hidden=10, gamma=0.9, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)\n",
    "\n",
    "#transition = init_transition()\n",
    "#agent = Agent(n_actions=n_actions, inp_size=np.array(transition).shape, device=device, hidden=10, agent_history_length=agent_history_length, memory_size=replay_memory_size, batch_size=batch_size, learning_rate=learning_rate)\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, eps_annealing_steps=eps_annealing_steps, eps_final=0.2, eps_final_step=0.02, replay_memory_start_size=start_learning, model=agent.main_dqn)\n",
    "\n",
    "step_counter = 0\n",
    "\n",
    "eps_rewards = []\n",
    "\n",
    "episode_lengths = []\n",
    "\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "    #agent.main_dqn.train()\n",
    "######## fill memory begins here\n",
    "    while (epoch_step < evaluate_every) or (step_counter < start_learning):\n",
    "        state = env.reset()\n",
    "        env.stepCounter = np.random.randint(len(env.referenceStreamline_ijk)-5)\n",
    "        env.state = TractographyState(env.referenceStreamline_ijk[env.stepCounter], env.interpolateDWIatState)\n",
    "        #transition = init_transition()\n",
    "        #referenceLine = env.referenceStreamline_ijk\n",
    "        episode_reward_sum = 0\n",
    "        terminal = False\n",
    "        #fill replay memory while interacting with env\n",
    "        #for episode_counter in range(max_episode_length):\n",
    "        episode_step_counter = 0\n",
    "        positive_run = 0\n",
    "        \n",
    "        dist = 0\n",
    "        influential_action = None\n",
    "        while not terminal:\n",
    "            # get action with epsilon-greedy strategy\n",
    "            #if dist < 0.1:\n",
    "            _, optimal_reward = get_random_good_action(state, env)\n",
    "                #print(influential_action)\n",
    "            #else:\n",
    "            #    influential_action = None\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)) #influential_action=influential_action)\n",
    "            #action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device))\n",
    "            \n",
    "            \n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            #print(reward)\n",
    "            episode_step_counter += 1\n",
    "            \n",
    "            terminal = False\n",
    "            \n",
    "            current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "            path_vector = (next_state.getCoordinate() - state.getCoordinate()).squeeze(0)\n",
    "            reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "            #    #print(path_vector, reference_vector)\n",
    "            cosine_sim = cos(path_vector, reference_vector)\n",
    "            dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2)\n",
    "            if dist < 0.1:\n",
    "                dist = 0\n",
    "            else:\n",
    "                dist = dist - 0.1\n",
    "            if dist > 3*0.81:\n",
    "                env.stepCounter -= 1\n",
    "            reward = cosine_sim - dist\n",
    "            reward = 1 - (optimal_reward - reward)\n",
    "            #reward = 1- (optimal_reward - dist)\n",
    "            #if reward == optimal_reward:\n",
    "            #    reward = 1\n",
    "            if action == 100 and dist < 0.1:\n",
    "                terminal = True\n",
    "            #print(\"From function: \", influential_action, optimal_reward)\n",
    "            #print(\"From scheduler: \", action, reward,  terminal)\n",
    "            #print(\"Cosine sim: \", cosine_sim)\n",
    "            #print(\"Dist: \", dist)\n",
    "            \n",
    "            if episode_step_counter >= 200:\n",
    "                terminal = True\n",
    "            \n",
    "            #print(episode_step_counter, action, reward, terminal)\n",
    "            #print(reward)\n",
    "            #if dist > 0.7: # cosine_sim < 0.4 or\n",
    "            #    terminal = True\n",
    "            #next_state = next_state[:2]\n",
    "            #next_transition = add_to_transition(next_state, transition)\n",
    "            \n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "\n",
    "            # accumulate reward for current episode\n",
    "            episode_reward_sum += reward\n",
    "\n",
    "\n",
    "            agent.replay_memory.add_experience(action=action,\n",
    "                                #state=np.array(transition),\n",
    "                                state = state.getValue(),\n",
    "                                reward=reward,\n",
    "                                #new_state=np.array(next_transition),\n",
    "                                new_state = next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "            #transition = next_transition\n",
    "\n",
    "\n",
    "\n",
    "            ####### optimization is happening here\n",
    "            if step_counter > start_learning:\n",
    "                #if reward > 0.:\n",
    "                #    print(\"reward was positive: \", reward)\n",
    "                loss = agent.optimize()\n",
    "\n",
    "\n",
    "            ####### target network update\n",
    "            if step_counter > start_learning and step_counter % network_update_every == 0:\n",
    "                #print(\"Update net\")\n",
    "                #print(agent.main_dqn(torch.tensor(state).to(device).unsqueeze(0)))\n",
    "                #print(agent.target_dqn(torch.tensor(state).to(device).unsqueeze(0)))\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "\n",
    "            # if episode ended before maximum step\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                episode_lengths.append(episode_step_counter)\n",
    "                #state = env.reset()[:2]\n",
    "                #transition = init_transition()\n",
    "                break\n",
    "\n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "\n",
    "        if len(eps_rewards) % 1 == 0:\n",
    "            #with open(path+'/logs/rewards.dat', 'a') as reward_file:\n",
    "                #print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])), file=reward_file)\n",
    "            print(\"{}, done {} episodes, {}, current eps {}\".format(step_counter, len(eps_rewards), np.mean(eps_rewards[-100:]), action_scheduler.eps_current), np.mean(episode_lengths[-100:]))\n",
    "    #torch.save(agent.main_dqn.state_dict(), path+'/checkpoints/fibre_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eps_rewards[-100:])))\n",
    "\n",
    "########## evaluation starting here\n",
    "    eval_rewards = []\n",
    "    episode_final = 0\n",
    "    #agent.main_dqn.eval()\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        #transition = init_transition()\n",
    "        #env.state = TractographyState(env.referenceStreamline_ijk[0], env.interpolateDWIatState)\n",
    "        #env.stepCounter = 0\n",
    "        \n",
    "        eval_episode_reward = 0\n",
    "        while eval_steps < max_episode_length:\n",
    "            _, optimal_reward = get_random_good_action(state, env)\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).unsqueeze(0).to(device), evaluation=True)\n",
    "            #action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "            \n",
    "            eval_steps += 1\n",
    "            \n",
    "            terminal = False\n",
    "            current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "            path_vector = (next_state.getCoordinate() - state.getCoordinate()).squeeze(0)\n",
    "            reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "            #    #print(path_vector, reference_vector)\n",
    "            cosine_sim = cos(path_vector, reference_vector)\n",
    "            dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2)\n",
    "            if dist < 0.1:\n",
    "                dist = 0\n",
    "            else:\n",
    "                dist = dist - 0.1\n",
    "            if dist > 3*0.81:\n",
    "                env.stepCounter -= 1\n",
    "            reward = cosine_sim - dist\n",
    "            reward = 1- (optimal_reward - reward)\n",
    "            #if reward == optimal_reward:\n",
    "            #    reward = 1\n",
    "            if action == 100 and env.rewardForTerminalState(next_state) < 0.1:\n",
    "                terminal = True\n",
    "\n",
    "            if episode_step_counter == 200:\n",
    "                terminal = True\n",
    "            \n",
    "            #if cosine_sim < 0.9:\n",
    "            #    terminal = True\n",
    "            \n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "            #transition = next_transition\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                if reward == 1.:\n",
    "                    print(reward)\n",
    "                    episode_final += 1\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))\n",
    "    print(\"{} of {} episodes ended close to / at the final state.\".format(episode_final, eval_runs))\n",
    "    #if np.mean(eval_rewards) > 500.:\n",
    "    #    torch.save(agent.main_dqn.state_dict(), 'trained_agents/multiple/fibre_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eval_rewards))+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path_vector.shape)\n",
    "print(reference_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.5291, device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "q_vals = agent.main_dqn(torch.FloatTensor(state.getValue()).unsqueeze(0).to(device))\n",
    "print(q_vals[0][80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 58 [47.44071643 74.71694658 25.95875275] [47.870224 74.80299  26.640089] tensor(-3.9633, dtype=torch.float64)\n",
      "1 58 [47.01120886 74.63089938 25.27741647] [47.286892 74.28585  26.460402] tensor(-15.2968, dtype=torch.float64)\n",
      "Evaluation score: -19.260128263643985\n"
     ]
    }
   ],
   "source": [
    "eval_rewards = []\n",
    "all_distances = []\n",
    "all_states = []\n",
    "#agent.main_dqn.eval()\n",
    "for _ in range(1):\n",
    "    eval_steps = 0\n",
    "    state = env.reset()    \n",
    "    #state = env.reset()\n",
    "    #print(state.getCoordinate())\n",
    "    all_states.append(state.getCoordinate())\n",
    "    #transition = init_transition()\n",
    "    #all_states.append(torch.tensor(list(transition)[:3]))\n",
    "    eval_episode_reward = 0\n",
    "    episode_final = 0\n",
    "    #print(env.referenceStreamline_ijk[:6])\n",
    "    \n",
    "    while eval_steps < max_episode_length:\n",
    "        action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).unsqueeze(0).to(device), evaluation=True)\n",
    "        #action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "        #action = torch.argmax(agent(torch.FloatTensor([np.array(transition)]).to(device)))\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "\n",
    "        current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "        path_vector = (next_state.getCoordinate() - state.getCoordinate()).squeeze(0)\n",
    "        reference_vector = env.referenceStreamline_ijk[current_index]-env.referenceStreamline_ijk[current_index-1]\n",
    "        #    #print(path_vector, reference_vector)\n",
    "        cosine_sim = cos(path_vector, reference_vector)\n",
    "        dist = torch.sum((env.referenceStreamline_ijk[current_index] - next_state.getCoordinate())**2) * 10\n",
    "        reward = cosine_sim - dist\n",
    "        reward = 1 - (optimal_reward - reward)\n",
    "        if dist > 3*0.81:\n",
    "            env.stepCounter -= 1\n",
    "        if action == 100 and reward == 1:\n",
    "            terminal = False\n",
    "            \n",
    "        #if cosine_sim < 0.7:\n",
    "        #    terminal = True\n",
    "        #next_state = next_state\n",
    "        #next_transition = add_to_transition(next_state, transition)\n",
    "        #reward = 1 + (1+(reward/10))\n",
    "        #if reward > 1:\n",
    "        #    reward = 1\n",
    "        #elif reward > 0.:\n",
    "        #    reward = 0\n",
    "        #else:\n",
    "        #    reward = -1\n",
    "        eval_episode_reward += reward\n",
    "        print(eval_steps, action, next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([eval_steps,len(env.referenceStreamline_ijk)-1])].numpy(), reward)\n",
    "        #print(eval_steps, action, next_state, env.referenceStreamline_ijk[np.min([eval_steps,len(env.referenceStreamline_ijk)-1])].numpy(), reward)\n",
    "        eval_steps += 1\n",
    "        if eval_steps == 200:\n",
    "            terminal = True\n",
    "        all_distances.append(reward)\n",
    "        all_states.append(next_state.getCoordinate())\n",
    "        #all_states.append(next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        #transition = next_transition\n",
    "        if terminal:\n",
    "            terminal = False\n",
    "            #if reward > 0.9:\n",
    "            #    episode_final += 1\n",
    "            break\n",
    "\n",
    "    eval_rewards.append(eval_episode_reward)\n",
    "\n",
    "print(\"Evaluation score:\", np.min(eval_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "states, actions, rewards, new_states, terminal_flags = agent.replay_memory.get_minibatch()\n",
    "print(np.array_equal(states[0], new_states[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere_dist(nextState):\n",
    "    current_index = np.min([env.stepCounter,len(env.referenceStreamline_ijk)-1])\n",
    "    x_dist = (nextState.getCoordinate()[0] - env.referenceStreamline_ijk[current_index][0]) **2\n",
    "    y_dist = (nextState.getCoordinate()[1] - env.referenceStreamline_ijk[current_index][1]) **2\n",
    "    z_dist = (nextState.getCoordinate()[2] - env.referenceStreamline_ijk[current_index][2]) **2\n",
    "    return x_dist + y_dist + z_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n"
     ]
    }
   ],
   "source": [
    "print(agent.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 11.394252873563218 tensor(0.1970, dtype=torch.float64) tensor(0.1970, dtype=torch.float64)\n",
      "67 11.394252873563218 tensor(0.2640, dtype=torch.float64) tensor(0.2640, dtype=torch.float64)\n",
      "72 11.394252873563218 tensor(0.2650, dtype=torch.float64) tensor(0.2650, dtype=torch.float64)\n",
      "75 11.394252873563218 tensor(0.1352, dtype=torch.float64) tensor(0.1352, dtype=torch.float64)\n",
      "80 11.394252873563218 tensor(0.0623, dtype=torch.float64) tensor(0.0623, dtype=torch.float64)\n",
      "88 11.394252873563218 tensor(0.0726, dtype=torch.float64) tensor(0.0726, dtype=torch.float64)\n",
      "93 11.394252873563218 tensor(0.1499, dtype=torch.float64) tensor(0.1499, dtype=torch.float64)\n",
      "96 11.394252873563218 tensor(0.1986, dtype=torch.float64) tensor(0.1986, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_actions):\n",
    "    state = env.reset()\n",
    "    next_state, reward, done = env.step(i)\n",
    "    s_dist = sphere_dist(next_state)\n",
    "    old_dist = torch.sum((env.referenceStreamline_ijk[env.stepCounter] - next_state.getCoordinate())**2)\n",
    "    if s_dist <= 0.52**2:\n",
    "        print(i, reward, s_dist, old_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 77.07567091 108.90497243  91.49815967] -100\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done = env.step(75)\n",
    "print(next_state.getCoordinate(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.283401924575482, 102.46417647849394, 66.32755479299973]\n"
     ]
    }
   ],
   "source": [
    "print(list(transition)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 24.1166, 103.8659,  64.9889])\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "#referenceLine = env.referenceStreamline_ijk\n",
    "print(state.getCoordinate())\n",
    "#print(referenceLine[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 23.13776944 102.38550419  64.06677327] [ 30.125023 102.08756   66.46997 ] -100\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done = env.step(74)\n",
    "print(next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter, len(referenceLine)])].numpy(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "optimal_steps =  [80, 88, 54, 96, 100, 67, 83, 75, 83, 75, 100, 83, 70, 67, 59, 100, 67, 59, 59, 59, 51, 100, 59, 56, 51, 61, 100, 66, 71, 66, 71, 71, 100, 71, 71, 71, 71, 100, 92, 84, 84, 38, 100, 97, 97, 97, 38, 100, 97, 30, 43, 43, 48, 100, 94, 81, 94, 35, 97, 100, 35, 22, 35, 35, 6, 100, 19, 3, 16, 3, 21, 100, 21, 16, 34, 21, 98, 34, 100, 39, 93, 39, 72, 72, 100, 69, 100]\n",
    "transition = init_transition()\n",
    "referenceLine = env.referenceStreamline_ijk\n",
    "print(len(referenceLine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 -0.09999999999999964\n"
     ]
    }
   ],
   "source": [
    "#action = action_scheduler.get_action(step_counter, torch.FloatTensor([np.array(transition)]).to(device), evaluation=True)\n",
    "next_state, reward, terminal = env.step(88)\n",
    "next_transition = add_to_transition(next_state, transition)\n",
    "print(action, reward)\n",
    "transition = next_transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  [ 73.651344 107.88106   93.29415 ]\n",
      "Next State:  [ 74.56195007 107.80595503  92.88775652]\n",
      "7.450580596923828e-09\n"
     ]
    }
   ],
   "source": [
    "# Debugging the reward function\n",
    "referenceLine = env.referenceStreamline_ijk\n",
    "stepCounter = 0\n",
    "maxSteps=200\n",
    "state = env.reset()\n",
    "print(\"State: \", state.getCoordinate().numpy())\n",
    "next_state, _, terminal = env.step(80)\n",
    "print(\"Next State: \", next_state.getCoordinate().numpy())\n",
    "\n",
    "def lineseg_dist(p, a, b):\n",
    "\n",
    "    # normalized tangent vector\n",
    "    d = np.divide(b - a, np.linalg.norm(b - a))\n",
    "\n",
    "    # signed parallel distance components\n",
    "    s = np.dot(a - p, d)\n",
    "    t = np.dot(p - b, d)\n",
    "\n",
    "    # clamped parallel distance\n",
    "    h = np.maximum.reduce([s, t, 0])\n",
    "\n",
    "    # perpendicular distance component\n",
    "    c = np.cross(p - a, d)\n",
    "\n",
    "    return np.hypot(h, np.linalg.norm(c))\n",
    "\n",
    "distance = lineseg_dist(referenceLine[86].numpy(), referenceLine[85].numpy(), referenceLine[86].numpy())\n",
    "print(distance)\n",
    "\n",
    "#print(\"Diff: \", next_state.getCoordinate().numpy()-state.getCoordinate().numpy())\n",
    "#qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "#print(\"Reference next state: \", referenceLine[stepCounter+1])\n",
    "#print(\"Diff to reference state: \", referenceLine[stepCounter+1]-next_state.getCoordinate().numpy())\n",
    "#distance = torch.min(torch.sum((referenceLine[np.min([stepCounter+1, maxSteps-1])] - qry_pt)**2, dim=1))\n",
    "#print(distance)\n",
    "#reward = torch.tanh(-distance+5.3)\n",
    "\n",
    "#if distance == -1:\n",
    "#    reward = 0.5\n",
    "#elif distance < 0.8:\n",
    "#    reward = 1+ (1-distance)\n",
    "#else:\n",
    "#    reward = np.max([1 - distance, -1])\n",
    "#print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19954145509142476\n",
      "0.03981679230000309\n",
      "0.07041062249999892\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "state = np.array([ 75.6, 107.95,  92.22])\n",
    "line = np.array([ 75.78847, 107.96255,  92.28433])\n",
    "\n",
    "print(np.linalg.norm(line - state, 2))\n",
    "\n",
    "sphere_dist = ((state[0] - line[0])**2 + (state[1]-line[1])**2 + (state[2]-line[2])**2)\n",
    "print(sphere_dist)\n",
    "normal_diff = np.sum(state-line)**2\n",
    "print(normal_diff)\n",
    "if sphere_dist < 0.2**2:\n",
    "    print(True)\n",
    "else:\n",
    "    print(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  100 Reward:  -0.6400096416473389\n",
      "Action:  80 Reward:  -0.3780286503520091\n",
      "Action:  75 Reward:  -0.17094774926353554\n",
      "Action:  80 Reward:  -0.06020208127816557\n",
      "Action:  75 Reward:  -0.023724592605490286\n",
      "Action:  100 Reward:  -0.023724592605490286\n",
      "Action:  62 Reward:  -0.031680450691759385\n",
      "Action:  75 Reward:  -0.10966306950569177\n",
      "Action:  83 Reward:  -0.2621558822401104\n",
      "Action:  75 Reward:  -0.4853828474102172\n",
      "Action:  83 Reward:  -0.7713330234194417\n",
      "Action:  100 Reward:  -0.7713330234194417\n",
      "Action:  83 Reward:  -1.158927472394806\n",
      "Action:  62 Reward:  -1.6468544226974164\n",
      "Action:  67 Reward:  -2.2078664481054533\n",
      "Action:  51 Reward:  -2.459970885856384\n",
      "Action:  67 Reward:  -3.0430611441644246\n",
      "Action:  100 Reward:  -3.0430611441644246\n",
      "Action:  59 Reward:  -3.7319386628026487\n",
      "Action:  59 Reward:  -4.531517914723884\n",
      "Action:  59 Reward:  -5.427621034792285\n",
      "Action:  100 Reward:  -5.427621034792285\n",
      "Action:  59 Reward:  -6.420164664306009\n",
      "Action:  56 Reward:  -7.21012573500936\n",
      "Action:  51 Reward:  -8.185668593823168\n",
      "Action:  56 Reward:  -9.264514952082235\n",
      "Action:  66 Reward:  -9.504376152250732\n",
      "Action:  100 Reward:  -9.504376152250732\n",
      "Action:  66 Reward:  -10.878316642589324\n",
      "Action:  71 Reward:  -11.684246290994608\n",
      "Action:  71 Reward:  -13.482882171224585\n",
      "Action:  79 Reward:  -15.013766894750662\n",
      "Action:  58 Reward:  -16.29778288166172\n",
      "Action:  100 Reward:  -16.29778288166172\n",
      "Action:  71 Reward:  -17.927867464472456\n",
      "Action:  71 Reward:  -19.616943063578457\n",
      "Action:  84 Reward:  -20.74882253322799\n",
      "Action:  71 Reward:  -22.68542229369059\n",
      "Action:  100 Reward:  -22.68542229369059\n",
      "Action:  92 Reward:  -24.094491148196\n",
      "Action:  84 Reward:  -26.036910111309087\n",
      "Action:  92 Reward:  -27.894909786068872\n",
      "Action:  97 Reward:  -29.22946434143986\n",
      "Action:  100 Reward:  -29.22946434143986\n",
      "Action:  97 Reward:  -30.93770942329201\n",
      "Action:  38 Reward:  -33.06379059780991\n",
      "Action:  97 Reward:  -35.48076469151409\n",
      "Action:  43 Reward:  -37.24656758094286\n",
      "Action:  38 Reward:  -39.87506653295411\n",
      "Action:  100 Reward:  -39.87506653295411\n",
      "Action:  43 Reward:  -42.07728895704025\n",
      "Action:  43 Reward:  -44.59950388329224\n",
      "Action:  89 Reward:  -46.525702788416744\n",
      "Action:  48 Reward:  -46.51577793318299\n",
      "Action:  100 Reward:  -46.51577793318299\n",
      "Action:  94 Reward:  -44.74553361569235\n",
      "Action:  81 Reward:  -46.71395822922152\n",
      "Action:  48 Reward:  -50.26025222152221\n",
      "Action:  43 Reward:  -53.83576866536265\n",
      "Action:  100 Reward:  -53.83576866536265\n",
      "Action:  35 Reward:  -57.7679173101252\n",
      "Action:  43 Reward:  -61.07110670119785\n",
      "Action:  35 Reward:  -64.29611023518841\n",
      "Action:  22 Reward:  -64.69258594426614\n",
      "Action:  27 Reward:  -67.94753644296517\n",
      "Action:  100 Reward:  -67.94753644296517\n",
      "Action:  6 Reward:  -68.60859514506335\n",
      "Action:  11 Reward:  -70.03265506052335\n",
      "Action:  3 Reward:  -69.6074991211795\n",
      "Action:  16 Reward:  -66.45768588248448\n",
      "Action:  100 Reward:  -66.45768588248448\n",
      "Action:  8 Reward:  -63.980960033928525\n",
      "Action:  29 Reward:  -63.97912872209463\n",
      "Action:  21 Reward:  -65.2877329760679\n",
      "Action:  21 Reward:  -71.00622766156863\n",
      "Action:  100 Reward:  -71.00622766156863\n",
      "Action:  34 Reward:  -75.30646175774287\n",
      "Action:  26 Reward:  -79.7251625711022\n",
      "Action:  26 Reward:  -85.7736700190284\n",
      "Action:  93 Reward:  -86.29779314738443\n",
      "Action:  39 Reward:  -89.76066176762737\n",
      "Action:  100 Reward:  -89.76066176762737\n",
      "Action:  93 Reward:  -92.76729682665923\n",
      "Action:  72 Reward:  -91.66762326274807\n",
      "Action:  77 Reward:  -91.52645978577566\n",
      "Action:  77 Reward:  -94.77110103827272\n",
      "Action:  100 Reward:  -94.77110103827272\n",
      "-3134.679829616308\n"
     ]
    }
   ],
   "source": [
    "#optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82, 100]\n",
    "eps_reward = 0\n",
    "state = env.reset()\n",
    "for i in optimal_steps:\n",
    "    next_state, reward, terminal = env.step(i)\n",
    "    state = next_state\n",
    "    eps_reward += reward.item()\n",
    "    print(\"Action: \", i, \"Reward: \", reward.item())\n",
    "print(eps_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init environment..\n",
      "Loading precomputed streamlines (data/HCP307200_DTI_smallSet.vtk) for ID 100307\n",
      "..done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Init environment..\")\n",
    "env = RLTe.RLtractEnvironment(device = 'cpu')\n",
    "print(\"..done!\")\n",
    "n_actions = env.action_space.n\n",
    "#print(n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([87, 3])\n"
     ]
    }
   ],
   "source": [
    "referenceLine = env.referenceStreamline_ijk\n",
    "print(referenceLine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([47.8702, 74.8030, 26.6401])\n",
      "tensor([47.8702, 74.8030, 26.6401])\n"
     ]
    }
   ],
   "source": [
    "print(referenceLine[0])\n",
    "state = TractographyState(referenceLine[0], env.interpolateDWIatState)\n",
    "print(state.getCoordinate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  0 Action:  41 Distance:  tensor(1.3545, dtype=torch.float64)\n",
      "Step:  0 Action:  66 Distance:  tensor(1.2187, dtype=torch.float64)\n",
      "Step:  0 Action:  74 Distance:  tensor(1.5895, dtype=torch.float64)\n",
      "Step:  0 Action:  79 Distance:  tensor(1.4856, dtype=torch.float64)\n",
      "Step:  0 Action:  82 Distance:  tensor(1.2496, dtype=torch.float64)\n",
      "Step:  0 Action:  87 Distance:  tensor(1.5944, dtype=torch.float64)\n",
      "Step:  0 Action:  95 Distance:  tensor(1.5184, dtype=torch.float64)\n",
      "0 []\n",
      "Step:  1 Action:  61 Distance:  tensor(1.3162, dtype=torch.float64)\n",
      "Step:  1 Action:  66 Distance:  tensor(1.2710, dtype=torch.float64)\n",
      "Step:  1 Action:  74 Distance:  tensor(1.6272, dtype=torch.float64)\n",
      "Step:  1 Action:  79 Distance:  tensor(1.3920, dtype=torch.float64)\n",
      "Step:  1 Action:  82 Distance:  tensor(1.4070, dtype=torch.float64)\n",
      "Step:  1 Action:  87 Distance:  tensor(1.4471, dtype=torch.float64)\n",
      "Step:  1 Action:  95 Distance:  tensor(1.5094, dtype=torch.float64)\n",
      "1 []\n",
      "Step:  2 Action:  41 Distance:  tensor(1.2821, dtype=torch.float64)\n",
      "Step:  2 Action:  61 Distance:  tensor(1.2315, dtype=torch.float64)\n",
      "Step:  2 Action:  74 Distance:  tensor(1.6046, dtype=torch.float64)\n",
      "Step:  2 Action:  79 Distance:  tensor(1.3549, dtype=torch.float64)\n",
      "Step:  2 Action:  82 Distance:  tensor(1.4040, dtype=torch.float64)\n",
      "Step:  2 Action:  87 Distance:  tensor(1.4843, dtype=torch.float64)\n",
      "Step:  2 Action:  95 Distance:  tensor(1.5651, dtype=torch.float64)\n",
      "2 []\n",
      "Step:  3 Action:  41 Distance:  tensor(1.2908, dtype=torch.float64)\n",
      "Step:  3 Action:  49 Distance:  tensor(1.2627, dtype=torch.float64)\n",
      "Step:  3 Action:  61 Distance:  tensor(1.2172, dtype=torch.float64)\n",
      "Step:  3 Action:  74 Distance:  tensor(1.5797, dtype=torch.float64)\n",
      "Step:  3 Action:  79 Distance:  tensor(1.2516, dtype=torch.float64)\n",
      "Step:  3 Action:  82 Distance:  tensor(1.4621, dtype=torch.float64)\n",
      "Step:  3 Action:  87 Distance:  tensor(1.4322, dtype=torch.float64)\n",
      "Step:  3 Action:  95 Distance:  tensor(1.6009, dtype=torch.float64)\n",
      "3 []\n",
      "Step:  4 Action:  49 Distance:  tensor(1.3226, dtype=torch.float64)\n",
      "Step:  4 Action:  61 Distance:  tensor(1.3563, dtype=torch.float64)\n",
      "Step:  4 Action:  74 Distance:  tensor(1.5702, dtype=torch.float64)\n",
      "Step:  4 Action:  82 Distance:  tensor(1.5784, dtype=torch.float64)\n",
      "Step:  4 Action:  87 Distance:  tensor(1.2502, dtype=torch.float64)\n",
      "Step:  4 Action:  95 Distance:  tensor(1.5545, dtype=torch.float64)\n",
      "4 []\n",
      "Step:  5 Action:  49 Distance:  tensor(1.2380, dtype=torch.float64)\n",
      "Step:  5 Action:  61 Distance:  tensor(1.4426, dtype=torch.float64)\n",
      "Step:  5 Action:  69 Distance:  tensor(1.2523, dtype=torch.float64)\n",
      "Step:  5 Action:  74 Distance:  tensor(1.5910, dtype=torch.float64)\n",
      "Step:  5 Action:  82 Distance:  tensor(1.5806, dtype=torch.float64)\n",
      "Step:  5 Action:  87 Distance:  tensor(1.2100, dtype=torch.float64)\n",
      "Step:  5 Action:  95 Distance:  tensor(1.4956, dtype=torch.float64)\n",
      "5 []\n",
      "Step:  6 Action:  61 Distance:  tensor(1.5273, dtype=torch.float64)\n",
      "Step:  6 Action:  66 Distance:  tensor(1.3657, dtype=torch.float64)\n",
      "Step:  6 Action:  69 Distance:  tensor(1.2500, dtype=torch.float64)\n",
      "Step:  6 Action:  74 Distance:  tensor(1.6194, dtype=torch.float64)\n",
      "Step:  6 Action:  79 Distance:  tensor(1.2759, dtype=torch.float64)\n",
      "Step:  6 Action:  82 Distance:  tensor(1.5115, dtype=torch.float64)\n",
      "Step:  6 Action:  87 Distance:  tensor(1.2051, dtype=torch.float64)\n",
      "Step:  6 Action:  95 Distance:  tensor(1.3869, dtype=torch.float64)\n",
      "6 []\n",
      "Step:  7 Action:  61 Distance:  tensor(1.5700, dtype=torch.float64)\n",
      "Step:  7 Action:  66 Distance:  tensor(1.4237, dtype=torch.float64)\n",
      "Step:  7 Action:  69 Distance:  tensor(1.2798, dtype=torch.float64)\n",
      "Step:  7 Action:  74 Distance:  tensor(1.5954, dtype=torch.float64)\n",
      "Step:  7 Action:  79 Distance:  tensor(1.2693, dtype=torch.float64)\n",
      "Step:  7 Action:  82 Distance:  tensor(1.4712, dtype=torch.float64)\n",
      "Step:  7 Action:  95 Distance:  tensor(1.2862, dtype=torch.float64)\n",
      "7 []\n",
      "Step:  8 Action:  53 Distance:  tensor(1.2474, dtype=torch.float64)\n",
      "Step:  8 Action:  61 Distance:  tensor(1.6099, dtype=torch.float64)\n",
      "Step:  8 Action:  66 Distance:  tensor(1.4011, dtype=torch.float64)\n",
      "Step:  8 Action:  69 Distance:  tensor(1.3809, dtype=torch.float64)\n",
      "Step:  8 Action:  74 Distance:  tensor(1.5549, dtype=torch.float64)\n",
      "Step:  8 Action:  82 Distance:  tensor(1.4958, dtype=torch.float64)\n",
      "Step:  8 Action:  95 Distance:  tensor(1.2269, dtype=torch.float64)\n",
      "8 []\n",
      "Step:  9 Action:  53 Distance:  tensor(1.2052, dtype=torch.float64)\n",
      "Step:  9 Action:  56 Distance:  tensor(1.2501, dtype=torch.float64)\n",
      "Step:  9 Action:  61 Distance:  tensor(1.6062, dtype=torch.float64)\n",
      "Step:  9 Action:  66 Distance:  tensor(1.2593, dtype=torch.float64)\n",
      "Step:  9 Action:  69 Distance:  tensor(1.5104, dtype=torch.float64)\n",
      "Step:  9 Action:  74 Distance:  tensor(1.4608, dtype=torch.float64)\n",
      "Step:  9 Action:  82 Distance:  tensor(1.5458, dtype=torch.float64)\n",
      "9 []\n",
      "Step:  10 Action:  56 Distance:  tensor(1.2257, dtype=torch.float64)\n",
      "Step:  10 Action:  61 Distance:  tensor(1.5655, dtype=torch.float64)\n",
      "Step:  10 Action:  69 Distance:  tensor(1.5418, dtype=torch.float64)\n",
      "Step:  10 Action:  74 Distance:  tensor(1.4100, dtype=torch.float64)\n",
      "Step:  10 Action:  82 Distance:  tensor(1.5741, dtype=torch.float64)\n",
      "Step:  10 Action:  90 Distance:  tensor(1.2631, dtype=torch.float64)\n",
      "10 []\n",
      "Step:  11 Action:  56 Distance:  tensor(1.2257, dtype=torch.float64)\n",
      "Step:  11 Action:  61 Distance:  tensor(1.5655, dtype=torch.float64)\n",
      "Step:  11 Action:  69 Distance:  tensor(1.5418, dtype=torch.float64)\n",
      "Step:  11 Action:  74 Distance:  tensor(1.4100, dtype=torch.float64)\n",
      "Step:  11 Action:  82 Distance:  tensor(1.5741, dtype=torch.float64)\n",
      "Step:  11 Action:  90 Distance:  tensor(1.2631, dtype=torch.float64)\n",
      "11 []\n",
      "Step:  12 Action:  56 Distance:  tensor(1.3080, dtype=torch.float64)\n",
      "Step:  12 Action:  61 Distance:  tensor(1.5581, dtype=torch.float64)\n",
      "Step:  12 Action:  69 Distance:  tensor(1.5947, dtype=torch.float64)\n",
      "Step:  12 Action:  74 Distance:  tensor(1.3253, dtype=torch.float64)\n",
      "Step:  12 Action:  77 Distance:  tensor(1.2564, dtype=torch.float64)\n",
      "Step:  12 Action:  82 Distance:  tensor(1.5506, dtype=torch.float64)\n",
      "Step:  12 Action:  90 Distance:  tensor(1.2967, dtype=torch.float64)\n",
      "12 []\n",
      "Step:  13 Action:  56 Distance:  tensor(1.3643, dtype=torch.float64)\n",
      "Step:  13 Action:  61 Distance:  tensor(1.5247, dtype=torch.float64)\n",
      "Step:  13 Action:  64 Distance:  tensor(1.2298, dtype=torch.float64)\n",
      "Step:  13 Action:  69 Distance:  tensor(1.6218, dtype=torch.float64)\n",
      "Step:  13 Action:  74 Distance:  tensor(1.2146, dtype=torch.float64)\n",
      "Step:  13 Action:  77 Distance:  tensor(1.3332, dtype=torch.float64)\n",
      "Step:  13 Action:  82 Distance:  tensor(1.5012, dtype=torch.float64)\n",
      "Step:  13 Action:  90 Distance:  tensor(1.3043, dtype=torch.float64)\n",
      "13 []\n",
      "Step:  14 Action:  56 Distance:  tensor(1.3236, dtype=torch.float64)\n",
      "Step:  14 Action:  61 Distance:  tensor(1.4642, dtype=torch.float64)\n",
      "Step:  14 Action:  64 Distance:  tensor(1.2667, dtype=torch.float64)\n",
      "Step:  14 Action:  69 Distance:  tensor(1.6333, dtype=torch.float64)\n",
      "Step:  14 Action:  77 Distance:  tensor(1.4185, dtype=torch.float64)\n",
      "Step:  14 Action:  82 Distance:  tensor(1.5103, dtype=torch.float64)\n",
      "Step:  14 Action:  90 Distance:  tensor(1.3923, dtype=torch.float64)\n",
      "14 []\n",
      "Step:  15 Action:  56 Distance:  tensor(1.2585, dtype=torch.float64)\n",
      "Step:  15 Action:  61 Distance:  tensor(1.3793, dtype=torch.float64)\n",
      "Step:  15 Action:  64 Distance:  tensor(1.2792, dtype=torch.float64)\n",
      "Step:  15 Action:  69 Distance:  tensor(1.6204, dtype=torch.float64)\n",
      "Step:  15 Action:  77 Distance:  tensor(1.4794, dtype=torch.float64)\n",
      "Step:  15 Action:  82 Distance:  tensor(1.4951, dtype=torch.float64)\n",
      "Step:  15 Action:  90 Distance:  tensor(1.4558, dtype=torch.float64)\n",
      "15 []\n",
      "Step:  16 Action:  61 Distance:  tensor(1.2885, dtype=torch.float64)\n",
      "Step:  16 Action:  64 Distance:  tensor(1.2906, dtype=torch.float64)\n",
      "Step:  16 Action:  69 Distance:  tensor(1.6013, dtype=torch.float64)\n",
      "Step:  16 Action:  77 Distance:  tensor(1.5375, dtype=torch.float64)\n",
      "Step:  16 Action:  82 Distance:  tensor(1.4737, dtype=torch.float64)\n",
      "Step:  16 Action:  90 Distance:  tensor(1.5160, dtype=torch.float64)\n",
      "16 []\n",
      "Step:  17 Action:  64 Distance:  tensor(1.2763, dtype=torch.float64)\n",
      "Step:  17 Action:  69 Distance:  tensor(1.5565, dtype=torch.float64)\n",
      "Step:  17 Action:  77 Distance:  tensor(1.5700, dtype=torch.float64)\n",
      "Step:  17 Action:  82 Distance:  tensor(1.4267, dtype=torch.float64)\n",
      "Step:  17 Action:  90 Distance:  tensor(1.5504, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  17 Action:  98 Distance:  tensor(1.2671, dtype=torch.float64)\n",
      "17 []\n",
      "Step:  18 Action:  64 Distance:  tensor(1.2763, dtype=torch.float64)\n",
      "Step:  18 Action:  69 Distance:  tensor(1.5565, dtype=torch.float64)\n",
      "Step:  18 Action:  77 Distance:  tensor(1.5700, dtype=torch.float64)\n",
      "Step:  18 Action:  82 Distance:  tensor(1.4267, dtype=torch.float64)\n",
      "Step:  18 Action:  90 Distance:  tensor(1.5504, dtype=torch.float64)\n",
      "Step:  18 Action:  98 Distance:  tensor(1.2671, dtype=torch.float64)\n",
      "18 []\n",
      "Step:  19 Action:  64 Distance:  tensor(1.2807, dtype=torch.float64)\n",
      "Step:  19 Action:  69 Distance:  tensor(1.5051, dtype=torch.float64)\n",
      "Step:  19 Action:  77 Distance:  tensor(1.6072, dtype=torch.float64)\n",
      "Step:  19 Action:  82 Distance:  tensor(1.3555, dtype=torch.float64)\n",
      "Step:  19 Action:  85 Distance:  tensor(1.2477, dtype=torch.float64)\n",
      "Step:  19 Action:  90 Distance:  tensor(1.5722, dtype=torch.float64)\n",
      "Step:  19 Action:  98 Distance:  tensor(1.3832, dtype=torch.float64)\n",
      "19 []\n",
      "Step:  20 Action:  44 Distance:  tensor(1.3385, dtype=torch.float64)\n",
      "Step:  20 Action:  49 Distance:  tensor(1.2286, dtype=torch.float64)\n",
      "Step:  20 Action:  69 Distance:  tensor(1.3823, dtype=torch.float64)\n",
      "Step:  20 Action:  77 Distance:  tensor(1.5818, dtype=torch.float64)\n",
      "Step:  20 Action:  82 Distance:  tensor(1.2872, dtype=torch.float64)\n",
      "Step:  20 Action:  85 Distance:  tensor(1.2530, dtype=torch.float64)\n",
      "Step:  20 Action:  90 Distance:  tensor(1.6020, dtype=torch.float64)\n",
      "Step:  20 Action:  98 Distance:  tensor(1.4867, dtype=torch.float64)\n",
      "20 []\n",
      "Step:  21 Action:  44 Distance:  tensor(1.4217, dtype=torch.float64)\n",
      "Step:  21 Action:  49 Distance:  tensor(1.2387, dtype=torch.float64)\n",
      "Step:  21 Action:  69 Distance:  tensor(1.2773, dtype=torch.float64)\n",
      "Step:  21 Action:  77 Distance:  tensor(1.5501, dtype=torch.float64)\n",
      "Step:  21 Action:  82 Distance:  tensor(1.2059, dtype=torch.float64)\n",
      "Step:  21 Action:  85 Distance:  tensor(1.2581, dtype=torch.float64)\n",
      "Step:  21 Action:  90 Distance:  tensor(1.5947, dtype=torch.float64)\n",
      "Step:  21 Action:  98 Distance:  tensor(1.5446, dtype=torch.float64)\n",
      "21 []\n",
      "Step:  22 Action:  44 Distance:  tensor(1.4126, dtype=torch.float64)\n",
      "Step:  22 Action:  69 Distance:  tensor(1.2116, dtype=torch.float64)\n",
      "Step:  22 Action:  77 Distance:  tensor(1.5488, dtype=torch.float64)\n",
      "Step:  22 Action:  85 Distance:  tensor(1.3282, dtype=torch.float64)\n",
      "Step:  22 Action:  90 Distance:  tensor(1.5470, dtype=torch.float64)\n",
      "Step:  22 Action:  98 Distance:  tensor(1.5829, dtype=torch.float64)\n",
      "22 []\n",
      "Step:  23 Action:  31 Distance:  tensor(1.2443, dtype=torch.float64)\n",
      "Step:  23 Action:  39 Distance:  tensor(1.2866, dtype=torch.float64)\n",
      "Step:  23 Action:  44 Distance:  tensor(1.3997, dtype=torch.float64)\n",
      "Step:  23 Action:  77 Distance:  tensor(1.5426, dtype=torch.float64)\n",
      "Step:  23 Action:  85 Distance:  tensor(1.3959, dtype=torch.float64)\n",
      "Step:  23 Action:  90 Distance:  tensor(1.4943, dtype=torch.float64)\n",
      "Step:  23 Action:  98 Distance:  tensor(1.6163, dtype=torch.float64)\n",
      "23 []\n",
      "Step:  24 Action:  31 Distance:  tensor(1.2934, dtype=torch.float64)\n",
      "Step:  24 Action:  39 Distance:  tensor(1.3726, dtype=torch.float64)\n",
      "Step:  24 Action:  44 Distance:  tensor(1.3641, dtype=torch.float64)\n",
      "Step:  24 Action:  77 Distance:  tensor(1.5137, dtype=torch.float64)\n",
      "Step:  24 Action:  85 Distance:  tensor(1.4408, dtype=torch.float64)\n",
      "Step:  24 Action:  90 Distance:  tensor(1.4189, dtype=torch.float64)\n",
      "Step:  24 Action:  98 Distance:  tensor(1.6270, dtype=torch.float64)\n",
      "24 []\n",
      "Step:  25 Action:  31 Distance:  tensor(1.3955, dtype=torch.float64)\n",
      "Step:  25 Action:  39 Distance:  tensor(1.4232, dtype=torch.float64)\n",
      "Step:  25 Action:  44 Distance:  tensor(1.4189, dtype=torch.float64)\n",
      "Step:  25 Action:  77 Distance:  tensor(1.4476, dtype=torch.float64)\n",
      "Step:  25 Action:  85 Distance:  tensor(1.4018, dtype=torch.float64)\n",
      "Step:  25 Action:  90 Distance:  tensor(1.3865, dtype=torch.float64)\n",
      "Step:  25 Action:  98 Distance:  tensor(1.6444, dtype=torch.float64)\n",
      "25 []\n",
      "Step:  26 Action:  23 Distance:  tensor(1.2010, dtype=torch.float64)\n",
      "Step:  26 Action:  31 Distance:  tensor(1.5438, dtype=torch.float64)\n",
      "Step:  26 Action:  39 Distance:  tensor(1.4724, dtype=torch.float64)\n",
      "Step:  26 Action:  44 Distance:  tensor(1.4670, dtype=torch.float64)\n",
      "Step:  26 Action:  77 Distance:  tensor(1.2564, dtype=torch.float64)\n",
      "Step:  26 Action:  85 Distance:  tensor(1.2705, dtype=torch.float64)\n",
      "Step:  26 Action:  90 Distance:  tensor(1.2592, dtype=torch.float64)\n",
      "Step:  26 Action:  98 Distance:  tensor(1.6194, dtype=torch.float64)\n",
      "26 []\n",
      "Step:  27 Action:  23 Distance:  tensor(1.2617, dtype=torch.float64)\n",
      "Step:  27 Action:  26 Distance:  tensor(1.3512, dtype=torch.float64)\n",
      "Step:  27 Action:  31 Distance:  tensor(1.6077, dtype=torch.float64)\n",
      "Step:  27 Action:  39 Distance:  tensor(1.5253, dtype=torch.float64)\n",
      "Step:  27 Action:  44 Distance:  tensor(1.3890, dtype=torch.float64)\n",
      "Step:  27 Action:  98 Distance:  tensor(1.5487, dtype=torch.float64)\n",
      "27 []\n",
      "Step:  28 Action:  18 Distance:  tensor(1.2351, dtype=torch.float64)\n",
      "Step:  28 Action:  23 Distance:  tensor(1.2164, dtype=torch.float64)\n",
      "Step:  28 Action:  26 Distance:  tensor(1.4285, dtype=torch.float64)\n",
      "Step:  28 Action:  31 Distance:  tensor(1.6014, dtype=torch.float64)\n",
      "Step:  28 Action:  39 Distance:  tensor(1.5558, dtype=torch.float64)\n",
      "Step:  28 Action:  44 Distance:  tensor(1.2939, dtype=torch.float64)\n",
      "Step:  28 Action:  98 Distance:  tensor(1.4966, dtype=torch.float64)\n",
      "28 []\n",
      "Step:  29 Action:  18 Distance:  tensor(1.3395, dtype=torch.float64)\n",
      "Step:  29 Action:  23 Distance:  tensor(1.2725, dtype=torch.float64)\n",
      "Step:  29 Action:  26 Distance:  tensor(1.4920, dtype=torch.float64)\n",
      "Step:  29 Action:  31 Distance:  tensor(1.6207, dtype=torch.float64)\n",
      "Step:  29 Action:  39 Distance:  tensor(1.5335, dtype=torch.float64)\n",
      "Step:  29 Action:  44 Distance:  tensor(1.2629, dtype=torch.float64)\n",
      "Step:  29 Action:  98 Distance:  tensor(1.4319, dtype=torch.float64)\n",
      "29 []\n",
      "Step:  30 Action:  18 Distance:  tensor(1.4184, dtype=torch.float64)\n",
      "Step:  30 Action:  23 Distance:  tensor(1.3032, dtype=torch.float64)\n",
      "Step:  30 Action:  26 Distance:  tensor(1.5302, dtype=torch.float64)\n",
      "Step:  30 Action:  31 Distance:  tensor(1.6147, dtype=torch.float64)\n",
      "Step:  30 Action:  39 Distance:  tensor(1.4859, dtype=torch.float64)\n",
      "Step:  30 Action:  44 Distance:  tensor(1.2065, dtype=torch.float64)\n",
      "Step:  30 Action:  98 Distance:  tensor(1.3418, dtype=torch.float64)\n",
      "30 []\n",
      "Step:  31 Action:  13 Distance:  tensor(1.2379, dtype=torch.float64)\n",
      "Step:  31 Action:  18 Distance:  tensor(1.4916, dtype=torch.float64)\n",
      "Step:  31 Action:  23 Distance:  tensor(1.3284, dtype=torch.float64)\n",
      "Step:  31 Action:  26 Distance:  tensor(1.5638, dtype=torch.float64)\n",
      "Step:  31 Action:  31 Distance:  tensor(1.6021, dtype=torch.float64)\n",
      "Step:  31 Action:  39 Distance:  tensor(1.4352, dtype=torch.float64)\n",
      "Step:  31 Action:  98 Distance:  tensor(1.2489, dtype=torch.float64)\n",
      "31 []\n",
      "Step:  32 Action:  0 Distance:  tensor(1.2495, dtype=torch.float64)\n",
      "Step:  32 Action:  13 Distance:  tensor(1.4117, dtype=torch.float64)\n",
      "Step:  32 Action:  18 Distance:  tensor(1.5820, dtype=torch.float64)\n",
      "Step:  32 Action:  23 Distance:  tensor(1.3226, dtype=torch.float64)\n",
      "Step:  32 Action:  26 Distance:  tensor(1.5775, dtype=torch.float64)\n",
      "Step:  32 Action:  31 Distance:  tensor(1.5211, dtype=torch.float64)\n",
      "Step:  32 Action:  39 Distance:  tensor(1.2828, dtype=torch.float64)\n",
      "32 []\n",
      "Step:  33 Action:  0 Distance:  tensor(1.4203, dtype=torch.float64)\n",
      "Step:  33 Action:  5 Distance:  tensor(1.3761, dtype=torch.float64)\n",
      "Step:  33 Action:  10 Distance:  tensor(1.2212, dtype=torch.float64)\n",
      "Step:  33 Action:  13 Distance:  tensor(1.5318, dtype=torch.float64)\n",
      "Step:  33 Action:  18 Distance:  tensor(1.6109, dtype=torch.float64)\n",
      "Step:  33 Action:  23 Distance:  tensor(1.2563, dtype=torch.float64)\n",
      "Step:  33 Action:  26 Distance:  tensor(1.5325, dtype=torch.float64)\n",
      "Step:  33 Action:  31 Distance:  tensor(1.3783, dtype=torch.float64)\n",
      "33 []\n",
      "Step:  34 Action:  0 Distance:  tensor(1.5741, dtype=torch.float64)\n",
      "Step:  34 Action:  5 Distance:  tensor(1.4806, dtype=torch.float64)\n",
      "Step:  34 Action:  8 Distance:  tensor(1.2429, dtype=torch.float64)\n",
      "Step:  34 Action:  10 Distance:  tensor(1.2187, dtype=torch.float64)\n",
      "Step:  34 Action:  13 Distance:  tensor(1.6071, dtype=torch.float64)\n",
      "Step:  34 Action:  18 Distance:  tensor(1.5614, dtype=torch.float64)\n",
      "Step:  34 Action:  21 Distance:  tensor(1.2155, dtype=torch.float64)\n",
      "Step:  34 Action:  26 Distance:  tensor(1.4623, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  34 Action:  34 Distance:  tensor(1.2363, dtype=torch.float64)\n",
      "34 []\n",
      "Step:  35 Action:  0 Distance:  tensor(1.6504, dtype=torch.float64)\n",
      "Step:  35 Action:  5 Distance:  tensor(1.4896, dtype=torch.float64)\n",
      "Step:  35 Action:  8 Distance:  tensor(1.3297, dtype=torch.float64)\n",
      "Step:  35 Action:  13 Distance:  tensor(1.6255, dtype=torch.float64)\n",
      "Step:  35 Action:  18 Distance:  tensor(1.5023, dtype=torch.float64)\n",
      "Step:  35 Action:  21 Distance:  tensor(1.3069, dtype=torch.float64)\n",
      "Step:  35 Action:  26 Distance:  tensor(1.4191, dtype=torch.float64)\n",
      "Step:  35 Action:  34 Distance:  tensor(1.2754, dtype=torch.float64)\n",
      "35 []\n",
      "Step:  36 Action:  0 Distance:  tensor(1.6504, dtype=torch.float64)\n",
      "Step:  36 Action:  5 Distance:  tensor(1.4574, dtype=torch.float64)\n",
      "Step:  36 Action:  8 Distance:  tensor(1.3736, dtype=torch.float64)\n",
      "Step:  36 Action:  13 Distance:  tensor(1.6424, dtype=torch.float64)\n",
      "Step:  36 Action:  18 Distance:  tensor(1.4433, dtype=torch.float64)\n",
      "Step:  36 Action:  21 Distance:  tensor(1.3961, dtype=torch.float64)\n",
      "Step:  36 Action:  26 Distance:  tensor(1.4171, dtype=torch.float64)\n",
      "Step:  36 Action:  34 Distance:  tensor(1.3541, dtype=torch.float64)\n",
      "36 []\n",
      "Step:  37 Action:  0 Distance:  tensor(1.6297, dtype=torch.float64)\n",
      "Step:  37 Action:  5 Distance:  tensor(1.3586, dtype=torch.float64)\n",
      "Step:  37 Action:  8 Distance:  tensor(1.3157, dtype=torch.float64)\n",
      "Step:  37 Action:  13 Distance:  tensor(1.6391, dtype=torch.float64)\n",
      "Step:  37 Action:  18 Distance:  tensor(1.3957, dtype=torch.float64)\n",
      "Step:  37 Action:  21 Distance:  tensor(1.4311, dtype=torch.float64)\n",
      "Step:  37 Action:  26 Distance:  tensor(1.4701, dtype=torch.float64)\n",
      "Step:  37 Action:  34 Distance:  tensor(1.4513, dtype=torch.float64)\n",
      "37 []\n",
      "Step:  38 Action:  0 Distance:  tensor(1.5064, dtype=torch.float64)\n",
      "Step:  38 Action:  5 Distance:  tensor(1.2675, dtype=torch.float64)\n",
      "Step:  38 Action:  8 Distance:  tensor(1.2816, dtype=torch.float64)\n",
      "Step:  38 Action:  13 Distance:  tensor(1.6227, dtype=torch.float64)\n",
      "Step:  38 Action:  18 Distance:  tensor(1.3224, dtype=torch.float64)\n",
      "Step:  38 Action:  21 Distance:  tensor(1.4697, dtype=torch.float64)\n",
      "Step:  38 Action:  26 Distance:  tensor(1.4794, dtype=torch.float64)\n",
      "Step:  38 Action:  34 Distance:  tensor(1.5213, dtype=torch.float64)\n",
      "Step:  38 Action:  47 Distance:  tensor(1.2133, dtype=torch.float64)\n",
      "38 []\n",
      "Step:  39 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  39 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  39 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  39 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  39 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  39 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  39 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  39 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "39 []\n",
      "Step:  40 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  40 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  40 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  40 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  40 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  40 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  40 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  40 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "40 []\n",
      "Step:  41 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  41 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  41 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  41 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  41 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  41 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  41 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  41 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "41 []\n",
      "Step:  42 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  42 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  42 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  42 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  42 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  42 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  42 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  42 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "42 []\n",
      "Step:  43 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  43 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  43 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  43 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  43 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  43 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  43 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  43 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "43 []\n",
      "Step:  44 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  44 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  44 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  44 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  44 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  44 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  44 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  44 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "44 []\n",
      "Step:  45 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  45 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  45 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  45 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  45 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  45 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  45 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  45 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "45 []\n",
      "Step:  46 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  46 Action:  5 Distance:  tensor(1.2129, dtype=torch.float64)\n",
      "Step:  46 Action:  8 Distance:  tensor(1.3191, dtype=torch.float64)\n",
      "Step:  46 Action:  13 Distance:  tensor(1.5965, dtype=torch.float64)\n",
      "Step:  46 Action:  18 Distance:  tensor(1.2075, dtype=torch.float64)\n",
      "Step:  46 Action:  21 Distance:  tensor(1.5323, dtype=torch.float64)\n",
      "Step:  46 Action:  26 Distance:  tensor(1.4036, dtype=torch.float64)\n",
      "Step:  46 Action:  34 Distance:  tensor(1.5428, dtype=torch.float64)\n",
      "46 []\n",
      "Step:  47 Action:  0 Distance:  tensor(1.5209, dtype=torch.float64)\n",
      "Step:  47 Action:  8 Distance:  tensor(1.2639, dtype=torch.float64)\n",
      "Step:  47 Action:  13 Distance:  tensor(1.5583, dtype=torch.float64)\n",
      "Step:  47 Action:  21 Distance:  tensor(1.5480, dtype=torch.float64)\n",
      "Step:  47 Action:  26 Distance:  tensor(1.3944, dtype=torch.float64)\n",
      "Step:  47 Action:  34 Distance:  tensor(1.5913, dtype=torch.float64)\n",
      "Step:  47 Action:  42 Distance:  tensor(1.2956, dtype=torch.float64)\n",
      "Step:  47 Action:  47 Distance:  tensor(1.2660, dtype=torch.float64)\n",
      "47 []\n",
      "Step:  48 Action:  0 Distance:  tensor(1.3885, dtype=torch.float64)\n",
      "Step:  48 Action:  8 Distance:  tensor(1.2964, dtype=torch.float64)\n",
      "Step:  48 Action:  13 Distance:  tensor(1.5198, dtype=torch.float64)\n",
      "Step:  48 Action:  21 Distance:  tensor(1.5907, dtype=torch.float64)\n",
      "Step:  48 Action:  26 Distance:  tensor(1.3027, dtype=torch.float64)\n",
      "Step:  48 Action:  29 Distance:  tensor(1.2582, dtype=torch.float64)\n",
      "Step:  48 Action:  34 Distance:  tensor(1.5862, dtype=torch.float64)\n",
      "Step:  48 Action:  42 Distance:  tensor(1.3674, dtype=torch.float64)\n",
      "Step:  48 Action:  47 Distance:  tensor(1.2157, dtype=torch.float64)\n",
      "48 []\n",
      "Step:  49 Action:  0 Distance:  tensor(1.5321, dtype=torch.float64)\n",
      "Step:  49 Action:  8 Distance:  tensor(1.3821, dtype=torch.float64)\n",
      "Step:  49 Action:  13 Distance:  tensor(1.5176, dtype=torch.float64)\n",
      "Step:  49 Action:  21 Distance:  tensor(1.6158, dtype=torch.float64)\n",
      "Step:  49 Action:  26 Distance:  tensor(1.2182, dtype=torch.float64)\n",
      "Step:  49 Action:  29 Distance:  tensor(1.3106, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  49 Action:  34 Distance:  tensor(1.5309, dtype=torch.float64)\n",
      "Step:  49 Action:  42 Distance:  tensor(1.3322, dtype=torch.float64)\n",
      "49 []\n",
      "Step:  50 Action:  0 Distance:  tensor(1.5322, dtype=torch.float64)\n",
      "Step:  50 Action:  8 Distance:  tensor(1.3821, dtype=torch.float64)\n",
      "Step:  50 Action:  13 Distance:  tensor(1.5176, dtype=torch.float64)\n",
      "Step:  50 Action:  21 Distance:  tensor(1.6158, dtype=torch.float64)\n",
      "Step:  50 Action:  26 Distance:  tensor(1.2182, dtype=torch.float64)\n",
      "Step:  50 Action:  29 Distance:  tensor(1.3106, dtype=torch.float64)\n",
      "Step:  50 Action:  34 Distance:  tensor(1.5309, dtype=torch.float64)\n",
      "Step:  50 Action:  42 Distance:  tensor(1.3322, dtype=torch.float64)\n",
      "50 []\n",
      "Step:  51 Action:  0 Distance:  tensor(1.5321, dtype=torch.float64)\n",
      "Step:  51 Action:  5 Distance:  tensor(1.2367, dtype=torch.float64)\n",
      "Step:  51 Action:  8 Distance:  tensor(1.4490, dtype=torch.float64)\n",
      "Step:  51 Action:  13 Distance:  tensor(1.5675, dtype=torch.float64)\n",
      "Step:  51 Action:  21 Distance:  tensor(1.6079, dtype=torch.float64)\n",
      "Step:  51 Action:  26 Distance:  tensor(1.2407, dtype=torch.float64)\n",
      "Step:  51 Action:  29 Distance:  tensor(1.2570, dtype=torch.float64)\n",
      "Step:  51 Action:  34 Distance:  tensor(1.4905, dtype=torch.float64)\n",
      "Step:  51 Action:  42 Distance:  tensor(1.2329, dtype=torch.float64)\n",
      "51 []\n",
      "Step:  52 Action:  0 Distance:  tensor(1.6565, dtype=torch.float64)\n",
      "Step:  52 Action:  5 Distance:  tensor(1.3360, dtype=torch.float64)\n",
      "Step:  52 Action:  8 Distance:  tensor(1.4893, dtype=torch.float64)\n",
      "Step:  52 Action:  13 Distance:  tensor(1.5909, dtype=torch.float64)\n",
      "Step:  52 Action:  21 Distance:  tensor(1.5736, dtype=torch.float64)\n",
      "Step:  52 Action:  26 Distance:  tensor(1.2366, dtype=torch.float64)\n",
      "Step:  52 Action:  34 Distance:  tensor(1.4237, dtype=torch.float64)\n",
      "52 []\n",
      "Step:  53 Action:  0 Distance:  tensor(1.6565, dtype=torch.float64)\n",
      "Step:  53 Action:  5 Distance:  tensor(1.3360, dtype=torch.float64)\n",
      "Step:  53 Action:  8 Distance:  tensor(1.4893, dtype=torch.float64)\n",
      "Step:  53 Action:  13 Distance:  tensor(1.5909, dtype=torch.float64)\n",
      "Step:  53 Action:  21 Distance:  tensor(1.5736, dtype=torch.float64)\n",
      "Step:  53 Action:  26 Distance:  tensor(1.2366, dtype=torch.float64)\n",
      "Step:  53 Action:  34 Distance:  tensor(1.4237, dtype=torch.float64)\n",
      "53 []\n",
      "Step:  54 Action:  0 Distance:  tensor(1.6565, dtype=torch.float64)\n",
      "Step:  54 Action:  5 Distance:  tensor(1.4373, dtype=torch.float64)\n",
      "Step:  54 Action:  8 Distance:  tensor(1.5157, dtype=torch.float64)\n",
      "Step:  54 Action:  13 Distance:  tensor(1.6149, dtype=torch.float64)\n",
      "Step:  54 Action:  18 Distance:  tensor(1.2637, dtype=torch.float64)\n",
      "Step:  54 Action:  21 Distance:  tensor(1.5260, dtype=torch.float64)\n",
      "Step:  54 Action:  26 Distance:  tensor(1.2495, dtype=torch.float64)\n",
      "Step:  54 Action:  34 Distance:  tensor(1.3568, dtype=torch.float64)\n",
      "54 []\n",
      "Step:  55 Action:  0 Distance:  tensor(1.7441, dtype=torch.float64)\n",
      "Step:  55 Action:  5 Distance:  tensor(1.5640, dtype=torch.float64)\n",
      "Step:  55 Action:  8 Distance:  tensor(1.5016, dtype=torch.float64)\n",
      "Step:  55 Action:  13 Distance:  tensor(1.5992, dtype=torch.float64)\n",
      "Step:  55 Action:  18 Distance:  tensor(1.3915, dtype=torch.float64)\n",
      "Step:  55 Action:  21 Distance:  tensor(1.3772, dtype=torch.float64)\n",
      "Step:  55 Action:  26 Distance:  tensor(1.2197, dtype=torch.float64)\n",
      "55 []\n",
      "Step:  56 Action:  0 Distance:  tensor(1.7894, dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 58 is out of bounds for dimension 0 with size 58",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a7dce26ea5fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTractographyState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferenceLine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolateDWIatState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepCounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#if reward == -1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/RLtractEnvironment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m#    rewardNextState = rewardDistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mrewardNextState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewardForState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;31m#if rewardNextState < 0.:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m#    rewardNextState = -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/RLtractEnvironment.py\u001b[0m in \u001b[0;36mrewardForState\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mqry_pt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCoordinate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m#distance = torch.min(torch.sum( (self.referenceStreamline_ijk[np.max([self.stepCounter-1-2,0]):np.min([self.stepCounter-1+1,self.maxSteps])] - qry_pt)**2, dim =1 ))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferenceStreamline_ijk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepCounter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m86\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mqry_pt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;31m#reward = torch.tanh(-distance+5.3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 58 is out of bounds for dimension 0 with size 58"
     ]
    }
   ],
   "source": [
    "possible_actions = []\n",
    "past_state = env.reset()\n",
    "all_next_states = []\n",
    "for i in range(len(referenceLine)):\n",
    "    best_actions = []\n",
    "    next_states = []\n",
    "    for z in range(n_actions):\n",
    "        env.state = TractographyState(referenceLine[i], env.interpolateDWIatState)\n",
    "        next_state, reward, _ = env.step(z)\n",
    "        env.stepCounter = i\n",
    "        #if reward == -1:\n",
    "        #    reward = 0\n",
    "        #elif reward < 0.2:\n",
    "        if reward > 1.0:\n",
    "            print(\"Step: \", i, \"Action: \", z, \"Distance: \", reward)\n",
    "        #    reward = 1\n",
    "        #elif reward < 1.:\n",
    "        #    reward = 0\n",
    "        #else:\n",
    "        #    reward = -1\n",
    "        #if reward == 1:\n",
    "        #    best_actions.append(z)\n",
    "            #print(i, z, referenceLine[i].numpy(), next_state.getCoordinate().numpy(), reward)\n",
    "    print(i, best_actions)\n",
    "    #print(i, reward)\n",
    "    #if reward > 0.9:\n",
    "    #    best_actions.append(i)\n",
    "    possible_actions.append(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_distance = []\n",
    "optimal_steps = []#[100, 80, 75, 80, 75, 100, 62, 75, 83, 75, 83, 100, 83, 62, 67, 51, 67, 100, 59, 59, 59, 100, 59, 56, 51, 56, 66, 100, 66, 71, 71, 79, 58, 100, 71, 71, 84, 71, 100, 92, 84, 92, 97, 100, 97, 38, 97, 43, 38, 100, 43, 43, 89, 48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "last_state = env.reset()\n",
    "print(len(env.referenceStreamline_ijk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "Hey, hey, hey we finally stopped at the terminal state! :D\n",
      "[80, 88, 67, 75, 75, 62, 75, 83, 75, 83, 83, 83, 62, 67, 67, 54, 59, 59, 59, 59, 59, 59, 51, 56, 51, 56, 66, 66, 79, 71, 71, 66, 71, 71, 71, 71, 71, 84, 92, 84, 84, 92, 97, 97, 38, 97, 97, 38, 43, 43, 38, 35, 89, 48, 48, 73, 94, 35, 89, 43, 35, 22, 35, 35, 14, 14, 11, 3, 3, 16, 16, 21, 16, 21, 21, 34, 26, 47, 26, 39, 93, 39, 72, 72, 77, 77, 100]\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "while len(optimal_steps) < 87:\n",
    "    step_distance = []\n",
    "    for i in range(n_actions):\n",
    "        env.reset()\n",
    "        if len(optimal_steps)>0:\n",
    "            for z in range(len(optimal_steps)):\n",
    "                _,_,_ = env.step(optimal_steps[z])\n",
    "        next_state, _, terminal = env.step(i)\n",
    "        #distance = lineseg_dist(next_state.getCoordinate().numpy(), referenceLine[np.min([len(optimal_steps), 85])].numpy(), referenceLine[np.min([len(optimal_steps)+1, len(referenceLine)-1])].numpy())\n",
    "        #distance = ((next_state.getCoordinate()[0] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][0])**2 \\\n",
    "        #              + (next_state.getCoordinate()[1] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][1])**2 \\\n",
    "        #              + (next_state.getCoordinate()[2] - env.referenceStreamline_ijk[np.min([env.stepCounter, 58])][2])**2)\n",
    "        current_index = np.min([env.stepCounter, len(env.referenceStreamline_ijk)-1])\n",
    "        qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "        distance = torch.sum((env.referenceStreamline_ijk[current_index] - qry_pt)**2)\n",
    "        \n",
    "        step_distance.append(distance)\n",
    "    optimal_steps.append(np.argmin(step_distance))\n",
    "print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 0, stepWidth 0.81\n",
    "optimal_steps = [80, 88, 67, 75, 75, 62, 75, 83, 75, 83, 83, 83, 62, 67, 67, 54, 59, 59, 59, 59, 59, 59, 51, 56, 51, 56, 66, 66, 79, 71, 71, 66, 71, 71, 71, 71, 71, 84, 92, 84, 84, 92, 97, 97, 38, 97, 97, 38, 43, 43, 38, 35, 89, 48, 48, 73, 94, 35, 89, 43, 35, 22, 35, 35, 14, 14, 11, 3, 3, 16, 16, 21, 16, 21, 21, 34, 26, 47, 26, 39, 93, 39, 72, 72, 77, 77, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.81\n",
    "optimal_steps = [17, 30, 17, 30, 17, 30, 17, 1, 6, 3, 3, 3, 11, 16, 16, 24, 16, 24, 37, 24, 91, 45, 78, 78, 86, 99, 86, 86, 86, 70, 70, 65, 70, 65, 86, 65, 86, 99, 99, 40, 45, 45, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.79\n",
    "#optimal_steps =[17, 30, 17, 30, 17, 30, 17, 1, 6, 3, 3, 3, 11, 16, 16, 16, 24, 24, 24, 37, 24, 91, 78, 78, 99, 86, 86, 86, 86, 86, 70, 70, 70, 65, 65, 86, 86, 86, 99, 99, 99, 45, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 4, stepWidth 0.78\n",
    "#optimal_steps = [17, 30, 17, 30, 17, 30, 17, 1, 6, 1, 3, 11, 3, 16, 16, 24, 16, 24, 24, 37, 37, 45, 78, 78, 99, 86, 86, 78, 86, 86, 78, 70, 65, 65, 65, 86, 86, 86, 99, 99, 99, 45, 78]\n",
    "print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 30, 17, 30, 17, 30, 17, 1, 6, 3, 3, 3, 11, 16, 16, 16, 24, 24, 37, 24, 37, 78, 45, 78, 86, 86, 86, 86, 86, 78, 70, 65, 70, 65, 86, 65, 86, 86, 99, 32, 99, 45]\n"
     ]
    }
   ],
   "source": [
    "# Streamline index 4, stepWidth 0.8\n",
    "#optimal_steps = [17, 30, 17, 30, 17, 30, 17, 1, 6, 3, 3, 3, 11, 16, 16, 16, 24, 24, 37, 24, 37, 78, 45, 78, 86, 86, 86, 86, 86, 78, 70, 65, 70, 65, 86, 65, 86, 86, 99, 32, 99, 45]\n",
    "print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamline index 2\n",
    "optimal_steps = [3, 3, 100, 6, 3, 11, 3, 6, 100, 11, 6, 11, 3, 100, 19, 3, 11, 6, 16, 100, 3, 3, 3, 2, 11, 7, 18, 100, 15, 7, 2, 2, 100, 0, 10, 0, 2, 0, 100, 0, 0, 3, 0, 11, 100, 3, 3, 3, 3, 100, 3, 3, 1, 11, 100, 3, 3, 3, 16, 11, 100, 16, 29, 16, 42, 100, 21, 29, 21, 42, 21, 100, 34, 21, 13, 26, 100, 23, 18, 23, 31, 100, 44, 31, 31, 44, 44, 100, 44, 31, 36, 98, 15, 100, 23, 23, 44, 15, 44, 100, 15, 28, 15, 20, 36, 100, 20, 28, 20, 20, 100, 28, 28, 36, 49, 28, 100, 36, 49, 49, 90, 100, 49, 95, 95, 49, 46, 49, 38, 36, 25, 100, 28, 28, 33, 36, 41, 100, 20, 28, 12, 20, 20, 100, 7, 15, 7, 20, 10, 25, 100]\n",
    "#print(optimal_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Streamline with len 44 (index 4)\n",
    "#print(optimal_steps)\n",
    "optimal_steps = [17, 30, 100, 17, 30, 17, 17, 6, 0, 17, 37, 0, 0, 78, 24, 16, 24, 24, 100, 37, 45, 45, 70, 100, 99, 86, 86, 78, 94, 62, 100, 65, 70, 86, 65, 100, 86, 94, 99, 45, 99, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with line distance\n",
    "#optimal_steps = [80, 88, 54, 96, 46, 75, 75, 75, 83, 75, 83, 83, 62, 54, 1, 59, 54, 59, 59, 67, 56, 59, 51, 59, 61, 11, 53, 61, 66, 71, 71, 79, 58, 71, 71, 71, 21, 71, 84, 92, 84, 92, 97, 84, 43, 84, 30, 97, 47, 97, 43, 30, 89, 35, 94, 73, 48, 89, 22, 72, 43, 35, 22, 35, 35, 6, 19, 3, 16, 16, 66, 16, 8, 21, 29, 21, 26, 26, 93, 26, 93, 85, 35, 85, 72, 77, 100]\n",
    "optimal_steps = [100, 80, 75, 80, 75, 100, 62, 75, 83, 75, 83, 100, 83, 62, 67, 51, 67, 100, 59, 59, 59, 100, 59, 56, 51, 56, 66, 100, 66, 71, 71, 79, 58, 100, 71, 71, 84, 71, 100, 92, 84, 92, 97, 100, 97, 38, 97, 43, 38, 100, 43, 43, 89, 48, 100, 94, 81, 48, 43, 100, 35, 43, 35, 22, 27, 100, 6, 11, 3, 16, 100, 8, 29, 21, 21, 100, 34, 26, 26, 93, 39, 100, 93, 72, 77, 77, 101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82]\n",
    "print(optimal_steps) # <-- min of max distance reward streamline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82]\n"
     ]
    }
   ],
   "source": [
    "optimal_steps = [80, 75, 80, 75, 62, 75, 83, 96, 51, 24, 62, 62, 62, 77, 65, 64, 67, 59, 56, 83, 82, 54, 56, 53, 56, 38, 56, 84, 66, 71, 71, 64, 97, 84, 71, 71, 38, 51, 30, 92, 97, 84, 43, 79, 27, 46, 89, 25, 81, 25, 48, 43, 86, 48, 57, 14, 89, 43, 43, 19, 92, 14, 27, 9, 78, 4, 16, 3, 29, 3, 47, 6, 42, 21, 39, 5, 72, 34, 98, 88, 90, 75, 77, 59, 49, 32, 82]\n",
    "print(optimal_steps) # <-- min of max distance reward streamline 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80, 88, 54, 96, 59, 37, 54, 37, 67, 91, 62, 78, 64, 70, 64, 62, 69, 83, 56, 59, 59, 53, 42, 53, 53, 56, 79, 58, 87, 60, 87, 58, 92, 52, 46, 58, 38, 58, 30, 58, 38, 84, 17, 55, 30, 76, 25, 76, 17, 76, 17, 81, 30, 86, 48, 65, 27, 97, 14, 84, 6, 97, 14, 89, 3, 27, 8, 19, 13, 19, 13, 29, 8, 96, 2, 88, 31, 47, 26, 59, 5, 72, 72, 85, 61, 39]\n"
     ]
    }
   ],
   "source": [
    "optimal_steps = [80, 88, 54, 96, 59, 37, 54, 37, 67, 91, 62, 78, 64, 70, 64, 62, 69, 83, 56, 59, 59, 53, 42, 53, 53, 56, 79, 58, 87, 60, 87, 58, 92, 52, 46, 58, 38, 58, 30, 58, 38, 84, 17, 55, 30, 76, 25, 76, 17, 76, 17, 81, 30, 86, 48, 65, 27, 97, 14, 84, 6, 97, 14, 89, 3, 27, 8, 19, 13, 19, 13, 29, 8, 96, 2, 88, 31, 47, 26, 59, 5, 72, 72, 85, 61, 39]\n",
    "print(optimal_steps) # <-- min reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change optimal steps\n",
    "optimal_steps = [17, 30, 100, 17, 30, 17, 17, 6, 0, 6, 0, 0, 0, 78, 24, 16, 24, 24, 100, 37, 45, 45, 70, 100, 99, 86, 86, 78, 94, 62, 100, 65, 70, 86, 65, 100, 86, 94, 99, 45, 99, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 73.651344 107.88106   93.29415 ] tensor([ 73.6513, 107.8811,  93.2942])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimal_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-fc460226c6be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlen_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreferenceStreamline_ijk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mall_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCoordinate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimal_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#print(step, reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimal_steps' is not defined"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(env.state.getCoordinate().numpy(), env.referenceStreamline_ijk[0])\n",
    "step = 1\n",
    "all_distances = []\n",
    "all_states = []\n",
    "len_line = len(env.referenceStreamline_ijk)-1\n",
    "all_states.append(state.getCoordinate())\n",
    "for i in optimal_steps:\n",
    "    next_state, reward, terminal = env.step(i)\n",
    "    #print(step, reward)\n",
    "    #current_index = np.min([env.points_visited+1,len(env.referenceStreamline_ijk)-1])\n",
    "    #print(\"Reference Line at current index: \", env.referenceStreamline_ijk[current_index])\n",
    "    #distance = lineseg_dist(next_state.getCoordinate().numpy(), referenceLine[step-1].numpy(), referenceLine[np.min([step, len(referenceLine)-1])].numpy())\n",
    "    #distance = 2 + (distance/10)\n",
    "    #distance = ((next_state.getCoordinate()[0] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][0])**2 \\\n",
    "    #                  + (next_state.getCoordinate()[1] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][1])**2 \\\n",
    "    #                  + (next_state.getCoordinate()[2] - env.referenceStreamline_ijk[np.min([env.stepCounter, len_line])][2])**2)\n",
    "    current_index = np.min([env.stepCounter, len(env.referenceStreamline_ijk)-1])\n",
    "    qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "    distance = torch.sum((env.referenceStreamline_ijk[current_index] - qry_pt)**2)\n",
    "    \n",
    "    print(step, i, next_state.getCoordinate().numpy(), env.referenceStreamline_ijk[np.min([env.stepCounter,len_line])].numpy(), reward, -distance.item())\n",
    "    all_distances.append(distance)\n",
    "    all_states.append(next_state.getCoordinate())\n",
    "    #if distance < 0.71:\n",
    "    #    reward = 1 - distance\n",
    "    #    #print(reward)\n",
    "    #    if reward < 0.3:\n",
    "    #        reward = 1\n",
    "    step += 1\n",
    "\n",
    "print(np.min(all_distances), np.max(all_distances), np.sum(all_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 76.4527, 108.1185,  91.8665])\n",
      "tensor(108.1185)\n"
     ]
    }
   ],
   "source": [
    "print(env.referenceStreamline_ijk[4])\n",
    "print(env.referenceStreamline_ijk.T[1][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset to streamline 3/5\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([74.9079, 68.6542,  6.6088]) tensor([74.9079, 68.6542,  6.6088])\n",
      "tensor([75.0515, 68.9644,  7.3432], dtype=torch.float64) tensor([75.0415, 68.9379,  7.3448])\n",
      "tensor([75.1951, 69.2746,  8.0775], dtype=torch.float64) tensor([75.1012, 69.2937,  8.0589])\n",
      "tensor([75.0732, 69.6984,  8.7569], dtype=torch.float64) tensor([75.1610, 69.6495,  8.7729])\n",
      "tensor([75.2168, 70.0086,  9.4913], dtype=torch.float64) tensor([75.2207, 70.0053,  9.4869])\n",
      "tensor([75.3605, 70.3188, 10.2256], dtype=torch.float64) tensor([75.3279, 70.4488, 10.1440])\n",
      "tensor([75.5249, 70.8786, 10.7874], dtype=torch.float64) tensor([75.4352, 70.8923, 10.8012])\n",
      "tensor([75.4030, 71.3025, 11.4668], dtype=torch.float64) tensor([75.4949, 71.2481, 11.5152])\n",
      "tensor([75.5466, 71.6127, 12.2012], dtype=torch.float64) tensor([75.6022, 71.6915, 12.1723])\n",
      "tensor([75.6902, 71.9229, 12.9355], dtype=torch.float64) tensor([75.6619, 72.0473, 12.8864])\n",
      "tensor([75.5988, 72.5737, 13.4090], dtype=torch.float64) tensor([75.6980, 72.5632, 13.4967])\n",
      "tensor([75.7633, 73.1335, 13.9709], dtype=torch.float64) tensor([75.7341, 73.0791, 14.1071])\n",
      "tensor([75.6413, 73.5573, 14.6503], dtype=torch.float64) tensor([75.7701, 73.5950, 14.7175])\n",
      "tensor([75.8058, 74.1171, 15.2121], dtype=torch.float64) tensor([75.8062, 74.1109, 15.3278])\n",
      "tensor([75.6839, 74.5409, 15.8915], dtype=torch.float64) tensor([75.8422, 74.6268, 15.9382])\n",
      "tensor([75.8483, 75.1007, 16.4534], dtype=torch.float64) tensor([75.8783, 75.1427, 16.5486])\n",
      "tensor([75.9920, 75.4109, 17.1877], dtype=torch.float64) tensor([75.9855, 75.5862, 17.2057])\n",
      "tensor([76.1564, 75.9707, 17.7496], dtype=torch.float64) tensor([76.0928, 76.0297, 17.8629])\n",
      "tensor([76.0345, 76.3946, 18.4290], dtype=torch.float64) tensor([76.2000, 76.4732, 18.5200])\n",
      "tensor([76.4324, 76.7621, 19.0312], dtype=torch.float64) tensor([76.4056, 76.9388, 19.1372])\n",
      "tensor([76.5760, 77.0723, 19.7656], dtype=torch.float64) tensor([76.5868, 77.3026, 19.8263])\n",
      "tensor([76.7196, 77.3824, 20.4999], dtype=torch.float64) tensor([76.7203, 77.5862, 20.5623])\n",
      "tensor([76.8633, 77.6926, 21.2343], dtype=torch.float64) tensor([76.8539, 77.8699, 21.2983])\n",
      "tensor([77.0069, 78.0028, 21.9686], dtype=torch.float64) tensor([76.9996, 78.0494, 22.0641])\n",
      "tensor([76.8952, 78.1343, 22.7600], dtype=torch.float64) tensor([77.0545, 78.1756, 22.8522])\n",
      "tensor([77.0638, 78.1613, 23.5518], dtype=torch.float64) tensor([77.1093, 78.3018, 23.6403])\n",
      "tensor([77.2325, 78.1883, 24.3436], dtype=torch.float64) tensor([77.1743, 78.3133, 24.4375])\n",
      "tensor([77.2174, 78.0002, 25.1313], dtype=torch.float64) tensor([77.2496, 77.9811, 25.1614])\n",
      "tensor([77.3647, 77.5814, 25.8088], dtype=torch.float64) tensor([77.3263, 77.4440, 25.7493])\n",
      "tensor([77.3177, 76.9954, 26.3661], dtype=torch.float64) tensor([77.4029, 76.9069, 26.3372])\n",
      "tensor([77.4650, 76.5766, 27.0435], dtype=torch.float64) tensor([77.2996, 76.5694, 27.0552])\n",
      "tensor([77.1535, 76.4929, 27.7865], dtype=torch.float64) tensor([77.1912, 76.3470, 27.8160])\n",
      "tensor([77.1384, 76.3048, 28.5742], dtype=torch.float64) tensor([77.1715, 76.1842, 28.5990])\n",
      "tensor([77.2857, 75.8860, 29.2517], dtype=torch.float64) tensor([77.2455, 75.9670, 29.3653])\n",
      "tensor([77.4544, 75.9130, 30.0435], dtype=torch.float64) tensor([77.4148, 75.8119, 30.1317])\n",
      "tensor([77.4393, 75.7248, 30.8312], dtype=torch.float64) tensor([77.5840, 75.6568, 30.8980])\n",
      "tensor([77.7739, 75.5189, 31.5395], dtype=torch.float64) tensor([77.6571, 75.5543, 31.6881])\n",
      "tensor([77.9426, 75.5459, 32.3313], dtype=torch.float64) tensor([77.8233, 75.5124, 32.4695])\n",
      "tensor([77.9275, 75.3577, 33.1190], dtype=torch.float64) tensor([77.9895, 75.4705, 33.2509])\n",
      "tensor([78.0961, 75.3848, 33.9108], dtype=torch.float64) tensor([78.1556, 75.4286, 34.0324])\n",
      "tensor([78.2648, 75.4118, 34.7026], dtype=torch.float64) tensor([78.3218, 75.3867, 34.8138])\n",
      "tensor([78.4335, 75.4388, 35.4944], dtype=torch.float64) tensor([78.4792, 75.4569, 35.5950])\n",
      "tensor([78.5771, 75.7490, 36.2287], dtype=torch.float64) tensor([78.6250, 75.6365, 36.3608])\n",
      "tensor([78.7458, 75.7760, 37.0205], dtype=torch.float64) tensor([78.7585, 75.9202, 37.0968])\n",
      "tensor([78.8894, 76.0862, 37.7548], dtype=torch.float64) tensor([78.8921, 76.2038, 37.8328])\n",
      "tensor([79.0330, 76.3964, 38.4892], dtype=torch.float64) tensor([79.0733, 76.5676, 38.5219])\n",
      "tensor([79.1975, 76.9562, 39.0510], dtype=torch.float64) tensor([79.2544, 76.9314, 39.2110])\n",
      "tensor([79.3411, 77.2664, 39.7853], dtype=torch.float64) tensor([79.4356, 77.2952, 39.9001])\n",
      "tensor([79.4847, 77.5766, 40.5197], dtype=torch.float64) tensor([79.4953, 77.6510, 40.6141])\n",
      "tensor([79.6283, 77.8868, 41.2540], dtype=torch.float64) tensor([79.6289, 77.9347, 41.3501])\n",
      "tensor([79.7720, 78.1970, 41.9883], dtype=torch.float64) tensor([79.7625, 78.2183, 42.0861])\n",
      "tensor([79.9156, 78.5072, 42.7227], dtype=torch.float64) tensor([79.8960, 78.5020, 42.8221])\n",
      "tensor([79.8039, 78.6387, 43.5141], dtype=torch.float64) tensor([79.9359, 78.7429, 43.5840])\n",
      "tensor([79.9475, 78.9489, 44.2484], dtype=torch.float64) tensor([79.9758, 78.9837, 44.3458])\n",
      "tensor([80.0911, 79.2591, 44.9828], dtype=torch.float64) tensor([80.0356, 79.3395, 45.0598])\n",
      "tensor([80.2348, 79.5693, 45.7171], dtype=torch.float64) tensor([80.2167, 79.7033, 45.7489])\n",
      "tensor([80.3992, 80.1291, 46.2789], dtype=torch.float64) tensor([80.3979, 80.0671, 46.4380])\n",
      "tensor([80.5428, 80.4393, 47.0133], dtype=torch.float64) tensor([80.5791, 80.4309, 47.1270])\n",
      "tensor([80.6865, 80.7495, 47.7476], dtype=torch.float64) tensor([80.7602, 80.7947, 47.8161])\n",
      "tensor([80.8509, 81.3093, 48.3095], dtype=torch.float64) tensor([80.9658, 81.2604, 48.4333])\n",
      "tensor([81.2488, 81.6767, 48.9117], dtype=torch.float64) tensor([81.2837, 81.7186, 49.0069])\n",
      "tensor([81.6467, 82.0442, 49.5140], dtype=torch.float64) tensor([81.6016, 82.1768, 49.5804])\n",
      "tensor([82.0446, 82.4117, 50.1163], dtype=torch.float64) tensor([82.0211, 82.6052, 50.1101])\n",
      "tensor([82.4410, 83.0113, 50.4898], dtype=torch.float64) tensor([82.4406, 83.0337, 50.6397])\n",
      "tensor([82.8389, 83.3787, 51.0920], dtype=torch.float64) tensor([82.9928, 83.3187, 51.1434])\n",
      "tensor([83.4771, 83.4912, 51.5781], dtype=torch.float64) tensor([83.6211, 83.5545, 51.5789])\n",
      "tensor([84.0760, 83.8648, 51.9754], dtype=torch.float64) tensor([84.2494, 83.7903, 52.0145])\n",
      "tensor([84.8286, 83.8004, 52.2678], dtype=torch.float64) tensor([84.9416, 83.9694, 52.3732])\n",
      "tensor([85.4275, 84.1740, 52.6651], dtype=torch.float64) tensor([85.6339, 84.1485, 52.7320])\n",
      "tensor([86.0656, 84.2864, 53.1511], dtype=torch.float64) tensor([86.3261, 84.3276, 53.0908])\n",
      "tensor([86.8253, 84.4987, 53.3351], dtype=torch.float64) tensor([87.0183, 84.5068, 53.4496])\n",
      "tensor([87.4635, 84.6111, 53.8212], dtype=torch.float64) tensor([87.6663, 84.6348, 53.9010])\n",
      "tensor([88.2161, 84.5467, 54.1136], dtype=torch.float64) tensor([88.3712, 84.7040, 54.2728])\n",
      "tensor([88.9758, 84.7590, 54.2976], dtype=torch.float64) tensor([89.1131, 84.8213, 54.5483])\n",
      "tensor([89.6139, 84.8715, 54.7837], dtype=torch.float64) tensor([89.8181, 84.8904, 54.9201])\n",
      "tensor([90.1933, 84.6947, 55.3214], dtype=torch.float64) tensor([90.3453, 84.6596, 55.4757])\n",
      "tensor([90.7727, 84.5180, 55.8592], dtype=torch.float64) tensor([90.8480, 84.3259, 56.0010])\n",
      "tensor([91.1989, 84.0752, 56.3869], dtype=torch.float64) tensor([91.2836, 83.8824, 56.5045])\n",
      "tensor([91.6251, 83.6325, 56.9146], dtype=torch.float64) tensor([91.6366, 83.3615, 56.9986])\n",
      "tensor([91.8480, 82.9906, 57.3554], dtype=torch.float64) tensor([91.9971, 82.7743, 57.4050])\n",
      "tensor([92.0709, 82.3487, 57.7963], dtype=torch.float64) tensor([92.2664, 82.1258, 57.7883])\n",
      "tensor([92.5305, 81.7367, 58.0616], dtype=torch.float64) tensor([92.6291, 81.4843, 58.0996])\n",
      "tensor([92.9901, 81.1248, 58.3270], dtype=torch.float64) tensor([92.9918, 80.8427, 58.4109])\n",
      "tensor([93.1874, 80.3549, 58.4830], dtype=torch.float64) tensor([93.3526, 80.1598, 58.6193])\n",
      "tensor([93.6470, 79.7429, 58.7484], dtype=torch.float64) tensor([93.7133, 79.4768, 58.8277])\n",
      "tensor([93.8444, 78.9730, 58.9045], dtype=torch.float64) tensor([94.0740, 78.7938, 59.0361])\n",
      "tensor([94.3039, 78.3610, 59.1698], dtype=torch.float64) tensor([94.3412, 78.0615, 59.2157])\n",
      "tensor([94.5013, 77.5911, 59.3259], dtype=torch.float64) tensor([94.6084, 77.3291, 59.3953])\n",
      "tensor([94.6987, 76.8212, 59.4820], dtype=torch.float64) tensor([94.8756, 76.5968, 59.5749])\n",
      "tensor([94.8960, 76.0513, 59.6380], dtype=torch.float64) tensor([95.1428, 75.8644, 59.7545])\n",
      "tensor([95.3556, 75.4393, 59.9034], dtype=torch.float64) tensor([95.4100, 75.1321, 59.9341])\n",
      "tensor([95.5530, 74.6694, 60.0595], dtype=torch.float64) tensor([95.5819, 74.3928, 60.1869])\n",
      "tensor([95.7759, 74.0275, 60.5003], dtype=torch.float64) tensor([95.8519, 73.6955, 60.4714])\n",
      "tensor([95.9732, 73.2575, 60.6564], dtype=torch.float64) tensor([96.0245, 72.9989, 60.8249])\n",
      "tensor([96.1962, 72.6156, 61.0972], dtype=torch.float64) tensor([96.2938, 72.3504, 61.2082])\n",
      "tensor([96.4191, 71.9737, 61.5381], dtype=torch.float64) tensor([96.4664, 71.6538, 61.5617])\n",
      "tensor([96.3701, 71.2289, 61.8528], dtype=torch.float64) tensor([96.5391, 70.9719, 61.9737])\n",
      "tensor([96.5931, 70.5870, 62.2936], dtype=torch.float64) tensor([96.7118, 70.2753, 62.3272])\n",
      "tensor([96.8160, 69.9451, 62.7344], dtype=torch.float64) tensor([96.7844, 69.5935, 62.7392])\n",
      "tensor([96.7671, 69.2003, 63.0491], dtype=torch.float64) tensor([96.8571, 68.9116, 63.1513])\n",
      "tensor([96.7181, 68.4555, 63.3638], dtype=torch.float64) tensor([96.8297, 68.2003, 63.5164])\n",
      "tensor([96.6692, 67.7108, 63.6785], dtype=torch.float64) tensor([96.7041, 67.5243, 63.9253])\n",
      "tensor([96.6223, 67.1249, 64.2358], dtype=torch.float64) tensor([96.5785, 66.8482, 64.3342])\n",
      "tensor([96.5734, 66.3801, 64.5505], dtype=torch.float64) tensor([96.4529, 66.1722, 64.7431])\n",
      "tensor([96.2485, 65.8134, 65.0295], dtype=torch.float64) tensor([96.2314, 65.5445, 65.1868])\n",
      "tensor([95.9237, 65.2467, 65.5085], dtype=torch.float64) tensor([96.0099, 64.9168, 65.6306])\n",
      "tensor([95.8748, 64.5019, 65.8231], dtype=torch.float64) tensor([95.7884, 64.2891, 66.0744])\n",
      "tensor([95.5499, 63.9352, 66.3021], dtype=torch.float64) tensor([95.4713, 63.6600, 66.4535])\n",
      "tensor([95.2251, 63.3685, 66.7811], dtype=torch.float64) tensor([95.1541, 63.0310, 66.8325])\n",
      "tensor([94.9016, 62.6597, 67.0027], dtype=torch.float64) tensor([94.8370, 62.4019, 67.2116])\n",
      "tensor([94.5768, 62.0930, 67.4816], dtype=torch.float64) tensor([94.5198, 61.7728, 67.5907])\n",
      "tensor([94.2532, 61.3842, 67.7032], dtype=torch.float64) tensor([94.2052, 61.0929, 67.8713])\n",
      "tensor([93.9297, 60.6755, 67.9248], dtype=torch.float64) tensor([93.8905, 60.4131, 68.1519])\n",
      "tensor([93.8808, 59.9307, 68.2394], dtype=torch.float64) tensor([93.6701, 59.6838, 68.3959])\n",
      "tensor([93.5573, 59.2219, 68.4610], dtype=torch.float64) tensor([93.4497, 58.9545, 68.6400])\n",
      "tensor([93.5084, 58.4772, 68.7757], dtype=torch.float64) tensor([93.3240, 58.1902, 68.8402])\n",
      "tensor([93.1849, 57.7684, 68.9973], dtype=torch.float64) tensor([93.1983, 57.4259, 69.0405])\n",
      "tensor([93.1360, 57.0236, 69.3119], dtype=torch.float64) tensor([93.0726, 56.6617, 69.2408])\n",
      "tensor([93.0243, 56.2217, 69.3346], dtype=torch.float64) tensor([92.9469, 55.8974, 69.4410])\n",
      "tensor([92.9126, 55.4197, 69.3572], dtype=torch.float64) tensor([92.8257, 55.1111, 69.5248])\n",
      "tensor([92.8009, 54.6178, 69.3799], dtype=torch.float64) tensor([92.8001, 54.3123, 69.5607])\n",
      "tensor([92.6892, 53.8158, 69.4025], dtype=torch.float64) tensor([92.6942, 53.5203, 69.5218])\n",
      "tensor([92.5775, 53.0139, 69.4252], dtype=torch.float64) tensor([92.5883, 52.7283, 69.4829])\n",
      "tensor([92.4658, 52.2120, 69.4479], dtype=torch.float64) tensor([92.4042, 51.9580, 69.3699])\n",
      "tensor([92.3312, 51.4698, 69.1525], dtype=torch.float64) tensor([92.2202, 51.1877, 69.2568])\n",
      "tensor([91.9594, 50.7573, 69.0518], dtype=torch.float64) tensor([91.9415, 50.4407, 69.1914])\n",
      "tensor([91.6359, 50.0485, 69.2734], dtype=torch.float64) tensor([91.5722, 49.7312, 69.1735])\n",
      "tensor([91.2641, 49.3360, 69.1726], dtype=torch.float64) tensor([91.1155, 49.0750, 69.1997])\n",
      "tensor([90.7106, 48.7541, 69.2781], dtype=torch.float64) tensor([90.5707, 48.4919, 69.2567])\n",
      "tensor([90.1571, 48.1722, 69.3836], dtype=torch.float64) tensor([90.0131, 47.9396, 69.4118])\n",
      "tensor([89.6036, 47.5903, 69.4891], dtype=torch.float64) tensor([89.4481, 47.4342, 69.6674])\n",
      "tensor([89.0415, 47.1288, 69.8457], dtype=torch.float64) tensor([88.9573, 46.8970, 69.9998])\n",
      "tensor([88.7166, 46.5621, 70.3247], dtype=torch.float64) tensor([88.4665, 46.3597, 70.3321])\n",
      "tensor([88.1631, 45.9802, 70.4302], dtype=torch.float64) tensor([88.0605, 45.7443, 70.6426])\n",
      "tensor([87.8396, 45.2714, 70.6518], dtype=torch.float64) tensor([87.6618, 45.0829, 70.8515])\n",
      "tensor([87.5161, 44.5627, 70.8733], dtype=torch.float64) tensor([87.3471, 44.4030, 71.1321])\n",
      "tensor([87.1913, 43.9960, 71.3523], dtype=torch.float64) tensor([87.0324, 43.7231, 71.4127])\n",
      "tensor([86.8678, 43.2872, 71.5739], dtype=torch.float64) tensor([86.7178, 43.0432, 71.6933])\n",
      "tensor([86.5443, 42.5784, 71.7954], dtype=torch.float64) tensor([86.3117, 42.4278, 72.0037])\n",
      "tensor([85.9822, 42.1170, 72.1521], dtype=torch.float64) tensor([85.8209, 41.8906, 72.3361])\n",
      "tensor([85.6573, 41.5502, 72.6311], dtype=torch.float64) tensor([85.4140, 41.3331, 72.7405])\n",
      "tensor([85.0952, 41.0888, 72.9877], dtype=torch.float64) tensor([85.0108, 40.8447, 73.2294])\n",
      "tensor([84.7704, 40.5221, 73.4667], dtype=torch.float64) tensor([84.7028, 40.3502, 73.7777])\n",
      "tensor([84.5618, 40.1495, 74.1550], dtype=torch.float64) tensor([84.3949, 39.8558, 74.3260])\n",
      "tensor([84.2369, 39.5828, 74.6339], dtype=torch.float64) tensor([84.2807, 39.3211, 74.9101])\n",
      "tensor([84.1900, 38.9969, 75.1913], dtype=torch.float64) tensor([84.0753, 38.8421, 75.5170])\n",
      "tensor([83.9814, 38.6243, 75.8796], dtype=torch.float64) tensor([83.8699, 38.3631, 76.1239])\n",
      "tensor([83.6566, 38.0576, 76.3585], dtype=torch.float64) tensor([83.6645, 37.8840, 76.7308])\n",
      "tensor([83.4479, 37.6850, 77.0468], dtype=torch.float64) tensor([83.4590, 37.4050, 77.3377])\n",
      "tensor([83.4010, 37.0990, 77.6042], dtype=torch.float64) tensor([83.2536, 36.9260, 77.9446])\n",
      "tensor([82.9231, 36.8100, 78.1908], dtype=torch.float64) tensor([82.8617, 36.5164, 78.5091])\n",
      "tensor([82.7144, 36.4375, 78.8791], dtype=torch.float64) tensor([82.5670, 36.1024, 79.1270])\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64) tensor([82.3763, 35.7140, 79.7998])\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n",
      "tensor([82.5058, 36.0649, 79.5674], dtype=torch.float64)\n",
      "tensor([82.1810, 35.4982, 80.0464], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(all_states):\n",
    "    try:\n",
    "        print(x, env.referenceStreamline_ijk[i])\n",
    "    except IndexError:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<mpl_toolkits.mplot3d.art3d.Line3D at 0x2aab4b060400>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for i in range(len(states)):\n",
    "#    print(states[i], env.referenceStreamline_ijk[i])\n",
    "#    distance = ((states.T[0][i] - env.referenceStreamline_ijk.T[0][i])**2 \\\n",
    "#                      + (states.T[1][i] - env.referenceStreamline_ijk.T[1][i] )**2 \\\n",
    "#                      + (states.T[2][i] - env.referenceStreamline_ijk.T[2][i])**2)\n",
    "#    print(distance)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(env.referenceStreamline_ijk.T[0][:], env.referenceStreamline_ijk.T[1][:], env.referenceStreamline_ijk.T[2][:])\n",
    "ax.plot3D(states.T[0][:], states.T[1][:], states.T[2][:])\n",
    "#print(optimal_steps[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([47.8702, 47.2516, 46.8551], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "states = torch.stack(all_states)\n",
    "print(states.T[0][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 71.7731, 113.9662,  79.6186])\n"
     ]
    }
   ],
   "source": [
    "print(referenceLine[86])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 73.651344 107.88106   93.29415 ] tensor([ 73.6513, 107.8811,  93.2942])\n",
      "tensor(66.2049, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.state.getCoordinate().numpy(), referenceLine[0])\n",
    "step = 0\n",
    "#all_rewards = []\n",
    "eps_reward = 0\n",
    "for i in optimal_steps:\n",
    "    next_state, distance, terminal = env.step(i)\n",
    "    if distance < 0.71:\n",
    "        reward = 1 - distance\n",
    "        #print(reward)\n",
    "        if reward < 0.3:\n",
    "            reward = 1\n",
    "    eps_reward += reward\n",
    "    #all_rewards.append(reward)\n",
    "    step += 1\n",
    "print(eps_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    \n",
    "    next_state, distance, terminal = env.step(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  100 Step:  1 Coordinates:  [ 73.651344 107.88106   93.29415 ] [ 74.42344 107.87124  93.08491]\n",
      "Action:  67 Step:  2 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 75.16057  107.88882   92.774536]\n",
      "Action:  100 Step:  3 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 75.78847 107.96255  92.28433]\n",
      "Action:  100 Step:  4 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 76.45265  108.118454  91.86654 ]\n",
      "Action:  100 Step:  5 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 77.116844 108.27435   91.448746]\n",
      "Action:  100 Step:  6 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 77.739716 108.54131   91.02359 ]\n",
      "Action:  100 Step:  7 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 78.36259  108.80828   90.598434]\n",
      "Action:  100 Step:  8 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 78.996666 109.15176   90.25207 ]\n",
      "Action:  100 Step:  9 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 79.630745 109.495224  89.9057  ]\n",
      "Action:  100 Step:  10 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 80.264824 109.8387    89.55933 ]\n",
      "Action:  100 Step:  11 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 80.833374 110.28288   89.21371 ]\n",
      "Action:  100 Step:  12 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 81.32385 110.75597  88.79464]\n",
      "Action:  100 Step:  13 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 81.78852 111.07612  88.22755]\n",
      "Action:  100 Step:  14 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 82.26274  111.20639   87.596565]\n",
      "Action:  100 Step:  15 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 82.764885 111.12094   86.97968 ]\n",
      "Action:  100 Step:  16 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 83.17989 111.072    86.2975 ]\n",
      "Action:  100 Step:  17 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 83.60093  110.91725   85.635086]\n",
      "Action:  100 Step:  18 Coordinates:  [ 74.34808267 107.88351105  92.57683017] [ 84.02196  110.762505  84.97268 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9b9d7b386c25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/RLtractEnvironment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mpositionNextState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetCoordinate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepWidth\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maction_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mnextState\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTractographyState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositionNextState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolateDWIatState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnextState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/_state.py\u001b[0m in \u001b[0;36mgetValue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolatedDWI\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# interpolate DWI value at self.coordinate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolatedDWI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolFuncHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoordinate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolatedDWI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/envs/RLtractEnvironment.py\u001b[0m in \u001b[0;36minterpolateDWIatState\u001b[0;34m(self, stateCoordinates)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0minterpolated_dwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_interpolated_dwi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mras_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdwi_postprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/data/__init__.py\u001b[0m in \u001b[0;36mget_interpolated_dwi\u001b[0;34m(self, points, postprocessing, ignore_outside_points)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             result = postprocessing(result, self.data.b0, \n\u001b[0m\u001b[1;32m    289\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbvecs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                                  self.data.bvals)\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/data/postprocessing.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(dwi, b0, bvecs, bvals)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mdata_sh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspherical_harmonics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msh_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbvecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mdata_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_sh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fibre_tracking/deepFibreTracking/dfibert/data/postprocessing.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(dwi, _b0, bvecs, _bvals)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mreal_sh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_sym_sh_mrtrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msh_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_sphere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_sphere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0minv_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_pinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_sh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mdata_sh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdwi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/dipy/reconst/shm.py\u001b[0m in \u001b[0;36msmooth_pinv\u001b[0;34m(B, L)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \"\"\"\n\u001b[1;32m    662\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m     \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpinv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mpinv\u001b[0;34m(a, rcond, hermitian)\u001b[0m\n\u001b[1;32m   1959\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconjugate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1961\u001b[0;31m     \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhermitian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m     \u001b[0;31m# discard small singular values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/atari/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m         \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->DdD'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->ddd'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m         \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "terminal = False\n",
    "step = 0\n",
    "actions = []\n",
    "past_state = env.state\n",
    "step = 1\n",
    "while terminal != True:\n",
    "    for i in range(n_actions)\n",
    "    action = np.random.randint(n_actions)\n",
    "    next_state, reward, terminal = env.step(action)\n",
    "    if reward < 1:\n",
    "        actions.append(action)\n",
    "        past_state = next_state\n",
    "        print(\"Action: \", action, \"Step: \",step, \"Coordinates: \", next_state.getCoordinate().numpy(), referenceLine[step].numpy())\n",
    "        step += 1\n",
    "    else:\n",
    "        env.state = past_state\n",
    "        env.stepCounter = step\n",
    "    #action = np.random.choice(possible_actions[step])\n",
    "    #next_state, reward, terminal = env.step(action)\n",
    "    #step += 1\n",
    "\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 77.7397, 108.5413,  91.0236])\n",
      "tensor([ 78.3626, 108.8083,  90.5984])\n"
     ]
    }
   ],
   "source": [
    "print(referenceLine[6])\n",
    "print(referenceLine[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 78.1077, 108.7354,  91.6977], dtype=torch.float64)\n",
      "tensor([ 77.7615, 108.8643,  91.6973], dtype=torch.float64)\n",
      "tensor([ 77.8808, 108.4698,  91.6927], dtype=torch.float64)\n",
      "tensor([ 78.0768, 109.0850,  91.6268], dtype=torch.float64)\n",
      "tensor([ 77.5148, 108.5987,  91.6375], dtype=torch.float64)\n",
      "tensor([ 78.3125, 108.4477,  91.5947], dtype=torch.float64)\n",
      "tensor([ 77.7489, 109.2253,  91.5590], dtype=torch.float64)\n",
      "tensor([ 77.6419, 108.2420,  91.5700], dtype=torch.float64)\n",
      "tensor([ 78.4297, 108.8083,  91.5614], dtype=torch.float64)\n",
      "tensor([ 77.4379, 108.9717,  91.5654], dtype=torch.float64)\n",
      "tensor([ 78.0813, 108.1849,  91.5566], dtype=torch.float64)\n",
      "tensor([ 78.1025, 109.3932,  91.4139], dtype=torch.float64)\n",
      "tensor([ 77.3094, 108.3452,  91.4445], dtype=torch.float64)\n",
      "tensor([ 78.6147, 108.4838,  91.3841], dtype=torch.float64)\n",
      "tensor([ 77.4417, 109.2998,  91.3783], dtype=torch.float64)\n",
      "tensor([ 77.8415, 107.9787,  91.4083], dtype=torch.float64)\n",
      "tensor([ 78.3907, 109.1557,  91.4638], dtype=torch.float64)\n",
      "tensor([ 77.2027, 108.6996,  91.4375], dtype=torch.float64)\n",
      "tensor([ 78.4256, 108.1555,  91.3717], dtype=torch.float64)\n",
      "tensor([ 77.7865, 109.5055,  91.3048], dtype=torch.float64)\n",
      "tensor([ 77.4984, 108.0024,  91.3116], dtype=torch.float64)\n",
      "tensor([ 78.6872, 108.8408,  91.3203], dtype=torch.float64)\n",
      "tensor([ 77.1729, 109.0570,  91.3086], dtype=torch.float64)\n",
      "tensor([ 78.1746, 107.9095,  91.2645], dtype=torch.float64)\n",
      "tensor([ 78.3889, 109.4422,  91.1813], dtype=torch.float64)\n",
      "tensor([ 77.0434, 108.4461,  91.1692], dtype=torch.float64)\n",
      "tensor([ 78.6965, 108.2442,  91.1141], dtype=torch.float64)\n",
      "tensor([ 77.5030, 109.5370,  91.1019], dtype=torch.float64)\n",
      "tensor([ 77.5000, 107.8270,  90.9938], dtype=torch.float64)\n",
      "tensor([ 78.6388, 109.1633,  91.2107], dtype=torch.float64)\n",
      "tensor([ 76.9888, 108.7771,  91.1266], dtype=torch.float64)\n",
      "tensor([ 78.4668, 107.9465,  91.0478], dtype=torch.float64)\n",
      "tensor([ 78.0655, 109.6183,  91.0848], dtype=torch.float64)\n",
      "tensor([ 77.2055, 108.1323,  91.1605], dtype=torch.float64)\n",
      "tensor([ 78.8286, 108.6225,  91.0812], dtype=torch.float64)\n",
      "tensor([ 77.2105, 109.3476,  91.0498], dtype=torch.float64)\n",
      "tensor([ 77.8390, 107.7826,  91.1087], dtype=torch.float64)\n",
      "tensor([ 78.6632, 109.3198,  90.9072], dtype=torch.float64)\n",
      "tensor([ 76.9030, 108.6561,  90.7911], dtype=torch.float64)\n",
      "tensor([ 78.6834, 108.0832,  90.7689], dtype=torch.float64)\n",
      "tensor([ 77.7496, 109.6751,  90.8953], dtype=torch.float64)\n",
      "tensor([ 77.2161, 107.9836,  90.8505], dtype=torch.float64)\n",
      "tensor([ 78.8374, 108.9642,  90.9474], dtype=torch.float64)\n",
      "tensor([ 77.0033, 109.0777,  90.9567], dtype=torch.float64)\n",
      "tensor([ 78.1431, 107.7515,  90.9129], dtype=torch.float64)\n",
      "tensor([ 78.3585, 109.5817,  90.8446], dtype=torch.float64)\n",
      "tensor([ 76.9926, 108.2972,  90.8376], dtype=torch.float64)\n",
      "tensor([ 78.8549, 108.4211,  90.8108], dtype=torch.float64)\n",
      "tensor([ 77.3852, 109.5591,  90.7525], dtype=torch.float64)\n",
      "tensor([ 77.7615, 107.7120,  90.7482], dtype=torch.float64)\n",
      "tensor([ 77.6912, 108.6687,  89.7427], dtype=torch.float64)\n",
      "tensor([ 78.0374, 108.5397,  89.7432], dtype=torch.float64)\n",
      "tensor([ 77.9181, 108.9343,  89.7477], dtype=torch.float64)\n",
      "tensor([ 77.7221, 108.3191,  89.8136], dtype=torch.float64)\n",
      "tensor([ 78.2841, 108.8053,  89.8029], dtype=torch.float64)\n",
      "tensor([ 77.4863, 108.9563,  89.8458], dtype=torch.float64)\n",
      "tensor([ 78.0500, 108.1788,  89.8814], dtype=torch.float64)\n",
      "tensor([ 78.1570, 109.1620,  89.8705], dtype=torch.float64)\n",
      "tensor([ 77.3692, 108.5958,  89.8791], dtype=torch.float64)\n",
      "tensor([ 78.3610, 108.4323,  89.8751], dtype=torch.float64)\n",
      "tensor([ 77.7176, 109.2191,  89.8838], dtype=torch.float64)\n",
      "tensor([ 77.6964, 108.0109,  90.0266], dtype=torch.float64)\n",
      "tensor([ 78.4895, 109.0588,  89.9960], dtype=torch.float64)\n",
      "tensor([ 77.1842, 108.9202,  90.0563], dtype=torch.float64)\n",
      "tensor([ 78.3572, 108.1042,  90.0621], dtype=torch.float64)\n",
      "tensor([ 77.9574, 109.4254,  90.0322], dtype=torch.float64)\n",
      "tensor([ 77.4082, 108.2484,  89.9767], dtype=torch.float64)\n",
      "tensor([ 78.5962, 108.7045,  90.0029], dtype=torch.float64)\n",
      "tensor([ 77.3733, 109.2486,  90.0688], dtype=torch.float64)\n",
      "tensor([ 78.0123, 107.8986,  90.1357], dtype=torch.float64)\n",
      "tensor([ 78.3005, 109.4017,  90.1289], dtype=torch.float64)\n",
      "tensor([ 77.1116, 108.5632,  90.1201], dtype=torch.float64)\n",
      "tensor([ 78.6259, 108.3471,  90.1318], dtype=torch.float64)\n",
      "tensor([ 77.6242, 109.4945,  90.1760], dtype=torch.float64)\n",
      "tensor([ 77.4100, 107.9618,  90.2592], dtype=torch.float64)\n",
      "tensor([ 78.7555, 108.9580,  90.2712], dtype=torch.float64)\n",
      "75 [ 78.7555186  108.95801267  90.27122457] [ 78.36259  108.80828   90.598434] 0.2838811622505798\n",
      "tensor([ 77.1024, 109.1599,  90.3263], dtype=torch.float64)\n",
      "tensor([ 78.2959, 107.8671,  90.3386], dtype=torch.float64)\n",
      "tensor([ 78.2988, 109.5771,  90.4467], dtype=torch.float64)\n",
      "tensor([ 77.1600, 108.2408,  90.2298], dtype=torch.float64)\n",
      "tensor([ 78.8100, 108.6269,  90.3138], dtype=torch.float64)\n",
      "tensor([ 77.3321, 109.4575,  90.3926], dtype=torch.float64)\n",
      "tensor([ 77.7333, 107.7858,  90.3557], dtype=torch.float64)\n",
      "tensor([ 78.5934, 109.2718,  90.2799], dtype=torch.float64)\n",
      "tensor([ 76.9703, 108.7816,  90.3592], dtype=torch.float64)\n",
      "tensor([ 78.5884, 108.0565,  90.3906], dtype=torch.float64)\n",
      "tensor([ 77.9598, 109.6215,  90.3317], dtype=torch.float64)\n",
      "tensor([ 77.1356, 108.0842,  90.5333], dtype=torch.float64)\n",
      "tensor([ 78.8959, 108.7480,  90.6493], dtype=torch.float64)\n",
      "88 [ 78.89586077 108.74800996  90.64932975] [ 78.36259  108.80828   90.598434] 0.2906038664155419\n",
      "tensor([ 77.1154, 109.3209,  90.6715], dtype=torch.float64)\n",
      "tensor([ 78.0493, 107.7290,  90.5451], dtype=torch.float64)\n",
      "tensor([ 78.5828, 109.4204,  90.5900], dtype=torch.float64)\n",
      "tensor([ 76.9615, 108.4399,  90.4931], dtype=torch.float64)\n",
      "tensor([ 78.7955, 108.3264,  90.4838], dtype=torch.float64)\n",
      "tensor([ 77.6558, 109.6526,  90.5275], dtype=torch.float64)\n",
      "tensor([ 77.4404, 107.8224,  90.5959], dtype=torch.float64)\n",
      "tensor([ 78.8063, 109.1069,  90.6029], dtype=torch.float64)\n",
      "96 [ 78.80625982 109.10688665  90.60289128] [ 78.36259  108.80828   90.598434] 0.28603082585170475\n",
      "tensor([ 76.9440, 108.9829,  90.6297], dtype=torch.float64)\n",
      "tensor([ 78.4137, 107.8450,  90.6879], dtype=torch.float64)\n",
      "tensor([ 78.0373, 109.6921,  90.6923], dtype=torch.float64)\n",
      "100 [ 77.89944  108.702034  90.72022 ] [ 78.36259  108.80828   90.598434] 0.24062869\n"
     ]
    }
   ],
   "source": [
    "state = TractographyState(torch.Tensor([ 77.8994346, 108.7020324, 90.72022516]), env.interpolateDWIatState)\n",
    "for i in range(n_actions):\n",
    "    env.state = state\n",
    "    env.stepCounter -= 1\n",
    "    next_state, _, terminal = env.step(i)\n",
    "    qry_pt = next_state.getCoordinate().view(-1,3)\n",
    "    distance = torch.min(torch.sum((referenceLine[7] - qry_pt)**2, dim=1))\n",
    "    if distance < 0.3:\n",
    "        print(i, next_state.getCoordinate().numpy(), referenceLine[7].numpy(), distance.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(187.0214)\n",
      "[ 73.651344 107.88106   93.29415 ]\n",
      "-1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "next_state, reward, terminal = env.step(100)\n",
    "print(next_state.getCoordinate().numpy())\n",
    "print(reward)\n",
    "print(terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47, 75, 80, 88, 93, 96], [67, 75, 80, 88, 93], [62, 67, 75, 80], [62, 67, 75, 80, 83], [62, 67, 75, 80, 83], [62, 67, 75, 83, 96], [62, 67, 75, 83, 96], [62, 75, 83, 91, 96], [62, 75, 83, 91, 96], [62, 75, 83, 91, 96], [62, 70, 75, 83, 91, 96], [62, 70, 75, 78, 83, 91], [54, 57, 62, 67, 70, 75, 83], [54, 62, 67, 75], [54, 59, 67, 72], [51, 54, 59, 62, 67, 72], [51, 54, 59, 64, 67, 72], [51, 54, 59, 64, 67, 72], [51, 54, 59, 64, 67, 72], [51, 54, 56, 59, 64, 67, 72], [51, 54, 56, 59, 64, 67, 72], [51, 56, 59, 64], [51, 56, 59, 64], [51, 56, 59, 64], [50, 51, 53, 56, 59], [50, 51, 53, 56, 61, 66], [53, 58, 61, 66, 74, 79], [58, 66, 71, 74, 79], [58, 66, 71, 79], [58, 63, 66, 71, 79], [58, 63, 66, 71, 79], [58, 63, 66, 71, 79], [58, 63, 66, 71, 79], [58, 63, 66, 71, 79, 84], [58, 63, 71, 79, 84, 92], [58, 63, 71, 79, 84, 92], [63, 71, 79, 84, 92], [63, 71, 79, 84, 92], [38, 71, 79, 84, 92], [38, 63, 71, 84, 92, 97], [38, 71, 84, 92, 97], [38, 84, 92, 97], [38, 76, 84, 92, 97], [38, 76, 84, 92, 97], [38, 43, 84, 89, 97], [38, 43, 84, 89, 97], [30, 38, 43, 97], [30, 38, 43, 97], [30, 38, 43, 97], [22, 30, 38, 43, 97], [22, 30, 38, 43, 97], [22, 35, 43, 89, 97], [35, 43, 48, 89], [40, 48, 81, 89, 94], [40, 48, 73, 81, 86, 94], [73, 81, 86, 94], [40, 48, 81, 89, 94], [35, 43, 48, 89], [22, 35, 43, 89, 97], [22, 35, 43, 89, 97], [22, 30, 35, 43, 89], [22, 30, 35, 43, 89], [14, 22, 27, 35, 43], [14, 22, 27, 35], [6, 14, 19, 22, 27, 35], [6, 11, 14, 19, 27], [3, 6, 11, 19], [3, 6, 11, 16], [3, 8, 11, 16], [3, 8, 11, 16, 24, 29], [3, 8, 16, 21, 29], [8, 16, 21, 29], [8, 13, 16, 21, 29], [8, 13, 16, 21, 29], [21, 29, 34, 42], [13, 21, 26, 34, 47], [13, 18, 26, 31, 34, 39, 47], [26, 34, 39, 47, 93], [26, 34, 39, 47, 93], [26, 39, 47, 85, 93], [26, 39, 47, 85, 93], [39, 47, 72, 85, 93], [64, 72, 80, 85, 93], [64, 72, 77, 85, 93], [56, 64, 69, 72, 77, 85], [64, 69, 77, 85, 90, 98], [100]]\n"
     ]
    }
   ],
   "source": [
    "print(possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 71.5173, 114.6476,  79.9506])\n",
      "tensor([ 71.7731, 113.9662,  79.6186])\n",
      "[64, 69, 77, 85, 90, 98]\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n",
      "tensor([ 71.5173, 114.6476,  79.9506]) tensor(0.4643)\n"
     ]
    }
   ],
   "source": [
    "env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "print(env.state.getCoordinate())\n",
    "print(referenceLine[86])\n",
    "print(possible_actions[85])\n",
    "for i in possible_actions[85]:\n",
    "    env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "    env.stepCounter = 84\n",
    "    next_state, reward, _ = env.step(z)\n",
    "    print(next_state.getCoordinate(), reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 71.773056 113.966225  79.618576] [ 72.30127204 114.02878755  79.99932066] 0.27901215525974304\n",
      "[ 71.773056 113.966225  79.618576] [ 71.7609279  113.6971063   80.14330044] 0.2753356828217112\n",
      "[ 71.773056 113.966225  79.618576] [ 71.37937604 113.65758474  79.97858924] 0.15498393104601757\n",
      "[ 71.773056 113.966225  79.618576] [ 71.66780472 114.12438903  79.11184227] 0.2567791198339029\n",
      "[ 71.773056 113.966225  79.618576] [ 71.97880832 114.37794723  79.10548775] 0.26325960220840583\n",
      "[ 71.773056 113.966225  79.618576] [ 71.31423388 113.95652005  79.25698482] 0.2105177635246191\n",
      "[ 71.773056 113.966225  79.618576] [ 71.97504289 114.04982978  79.29253904] 0.10630013104166292\n",
      "[ 71.773056 113.966225  79.618576] [ 71.63016002 113.84417747  79.3660627 ] 0.06376299386222199\n",
      "[ 71.773056 113.966225  79.618576] [ 72.24376411 114.29266559  79.36222956] 0.2215660937612901\n",
      "[ 71.773056 113.966225  79.618576] [ 71.91375289 113.81268975  79.56895814] 0.023572970787664616\n",
      "[ 71.773056 113.966225  79.618576] [ 71.35118583 113.73139254  79.58605126] 0.1779744651043897\n",
      "[ 71.773056 113.966225  79.618576] [ 72.20619251 114.00206886  79.62102829] 0.18760720625139998\n",
      "[ 71.773056 113.966225  79.618576] [ 71.66712842 113.67455586  79.7755295 ] 0.08507069391326497\n",
      "[ 71.773056 113.966225  79.618576] [ 72.03154546 113.7906186   79.91830767] 0.0898390464981324\n"
     ]
    }
   ],
   "source": [
    "#env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "for i in range(n_actions):\n",
    "    env.reset()\n",
    "    env.state = TractographyState(referenceLine[85], env.interpolateDWIatState)\n",
    "    next_state, reward, _ = env.step(i)\n",
    "    distance = env.rewardForTerminalState(next_state)\n",
    "    if distance < 0.3:\n",
    "        print(referenceLine[86].numpy(), next_state.getCoordinate().numpy(), distance.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 71.7731, 113.9662,  79.6186])\n",
      "tensor(122.0777, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "distance = env.rewardForTerminalState(next_state)\n",
    "print(referenceLine[86])\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    env.state = TractographyState(torch.FloatTensor([ 74.64776812, 107.9270337, 93.22325858]), env.interpolateDWIatState)\n",
    "    next_state, reward, _ = env.step(i)\n",
    "    env.stepCounter = 2\n",
    "    if reward < 0.1:\n",
    "        reward = 1\n",
    "    elif reward < 0.5:\n",
    "        reward = 0\n",
    "    else:\n",
    "        reward = -1\n",
    "    if reward == 1:\n",
    "        #best_actions.append(i)\n",
    "        print(\"[{}]\".format(i), referenceLine[2].numpy(), next_state.getCoordinate().numpy(), reward)\n",
    "#print(best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(torch.FloatTensor(referenceLine[0]), env.interpolateDWIatState)\n",
    "coordinates = state.getCoordinate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(referenceLine[0])\n",
    "print(referenceLine[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(referenceLine[69], env.interpolateDWIatState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = env.reset().getValue().reshape(-1).shape[0]\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.FloatTensor(state.getValue()).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vals = agent.main_dqn(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state.getValue().shape)\n",
    "shape = state.getValue().shape\n",
    "shape = np.prod(np.array(shape))\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState(referenceLine[70], env.interpolateDWIatState)\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(-1,3)\n",
    "distance = torch.min(torch.sum( (referenceLine - qry_pt)**2, dim =1 ))\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance_terminal = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "\n",
    "#print(distance)\n",
    "#print(distance_terminal)\n",
    "reward = (torch.tanh(-distance+5.3) + 2*torch.tanh(-distance_terminal+5.3))/2\n",
    "print(reward)\n",
    "\n",
    "print(torch.tanh(-distance+5.3))\n",
    "print(torch.tanh(-distance_terminal+5.3))\n",
    "\n",
    "reward += 200/20 * reward.sign()\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tanh(-distance_terminal+5.3)+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TractographyState([32., 84., 94.], env.interpolateDWIatState)\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(-1,3)\n",
    "distance = torch.min(torch.sum( (referenceLine - qry_pt)**2, dim =1 ))\n",
    "print(torch.tanh(-distance+5.3))\n",
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "print(-distance)\n",
    "print(torch.tanh(-distance)+2)\n",
    "#print(torch.where(distance < env.maxL2dist_to_terminalState, 1, 0 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-1.5 + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_pt = torch.FloatTensor(state.getCoordinate()).view(3)\n",
    "distance = torch.sum( (referenceLine[-1,:] - qry_pt)**2 )\n",
    "print(round(-distance.item(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Init agent\")\n",
    "#memory = ReplayMemory(size=replay_memory_size)\n",
    "state = env.reset()\n",
    "agent = Agent(n_actions=n_actions, inp_size=state.getValue().shape, device=device, hidden=256, agent_history_length=agent_history_length, memory_size=replay_memory_size, learning_rate=learning_rate)\n",
    "\n",
    "print(\"Init epsilon-greedy action scheduler\")\n",
    "action_scheduler = Action_Scheduler(num_actions=n_actions, max_steps=max_steps, eps_annealing_steps=100000, replay_memory_start_size=replay_memory_size, model=agent.main_dqn)\n",
    "\n",
    "step_counter = 0\n",
    "    \n",
    "eps_rewards = []\n",
    "\n",
    "print(\"Start training...\")\n",
    "while step_counter < max_steps:\n",
    "    epoch_step = 0\n",
    "\n",
    "######## fill memory begins here\n",
    "    while epoch_step < evaluate_every:  # To Do implement evaluation\n",
    "        state = env.reset()\n",
    "        episode_reward_sum = 0\n",
    "        \n",
    "        #fill replay memory while interacting with env\n",
    "        for episode_counter in range(max_episode_length):\n",
    "            # get action with epsilon-greedy strategy       \n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).to(device).unsqueeze(0))\n",
    "                    \n",
    "            next_state, reward, terminal = env.step(action)\n",
    "\n",
    "            if reward >= 1:\n",
    "                reward = 10\n",
    "            elif reward > -0.05:\n",
    "                reward = 1\n",
    "            \n",
    "            if episode_counter == max_episode_length-1:\n",
    "                reward = -100\n",
    "                terminal = True\n",
    "            # increase counter\n",
    "            step_counter += 1\n",
    "            epoch_step += 1\n",
    "\n",
    "            # accumulate reward for current episode\n",
    "            episode_reward_sum += reward\n",
    "\n",
    "\n",
    "            agent.replay_memory.add_experience(action=action,\n",
    "                                state=state.getValue(),\n",
    "                                reward=reward,\n",
    "                                new_state=next_state.getValue(),\n",
    "                                terminal=terminal)\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        \n",
    "\n",
    "            ####### optimization is happening here\n",
    "            if step_counter > replay_memory_size:\n",
    "                loss = agent.optimize()\n",
    "\n",
    "\n",
    "            ####### target network update\n",
    "            if step_counter > replay_memory_size and step_counter % network_update_every == 0:\n",
    "                agent.target_dqn.load_state_dict(agent.main_dqn.state_dict())\n",
    "            \n",
    "            # if episode ended before maximum step\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                state = env.reset()\n",
    "                break\n",
    "                \n",
    "        eps_rewards.append(episode_reward_sum)\n",
    "        \n",
    "        if len(eps_rewards) % 10 == 0:\n",
    "            with open(path+'/logs/rewards.dat', 'a') as reward_file:\n",
    "                print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])), file=reward_file)\n",
    "            print(\"[{}] {}, {}\".format(len(eps_rewards), step_counter, np.mean(eps_rewards[-100:])) )\n",
    "    torch.save(agent.main_dqn.state_dict(), path+'/checkpoints/fibre_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(eps_rewards[-100:])))\n",
    "########## evaluation starting here\n",
    "    eval_rewards = []\n",
    "    for _ in range(eval_runs):\n",
    "        eval_steps = 0\n",
    "        state = env.reset()\n",
    "        eval_episode_reward = 0\n",
    "        while eval_steps < max_episode_length:\n",
    "            action = action_scheduler.get_action(step_counter, torch.FloatTensor(state.getValue()).to(device).unsqueeze(0), evaluation=True)\n",
    "\n",
    "            next_state, reward, terminal = env.step(action)\n",
    "\n",
    "            eval_steps += 1\n",
    "            eval_episode_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if terminal:\n",
    "                terminal = False\n",
    "                break\n",
    "\n",
    "        eval_rewards.append(eval_episode_reward)\n",
    "    \n",
    "    print(\"Evaluation score:\", np.mean(eval_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p 'checkpoints/'\n",
    "#torch.save(agent.main_dqn.state_dict(), 'checkpoints/fiber_agent_{}_reward_{:.2f}.pth'.format(step_counter, np.mean(rewards[-100:])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CONDA (atari)",
   "language": "python",
   "name": "atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
